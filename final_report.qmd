---
# Keep metadata here for HTML/Internal use
title: "Benchmark Algorithm of Ensemblist Methods on Spatial Data"
author:
  - "Bianco Andrea"
  - "Mancini Matteo"
  - "Mellot Rodrigue"

format:
  pdf:
    documentclass: article
    papersize: a4
    title-block-render: false
    geometry:
      - top=20mm
      - bottom=20mm
      - left=20mm
      - right=20mm
    number-sections: true
    colorlinks: true
    fig-pos: "H"
    include-in-header:
      text: |
        \let\maketitle\relax 
    header-includes:
    - \usepackage{algorithm}
    - \usepackage{algpseudocode}
    - \usepackage{amsmath}

execute:
  enabled: true
  echo: false
  warning: false
  message: false
---

\begin{titlepage}
\begin{center}
\includegraphics[width=0.7\textwidth]{ensai_logo.png}\\[1.5 cm] 
 
\large{\textsc{Smart Data Project}}
\rule{\linewidth}{0.4mm} \\[0.3cm]
{\Large\bfseries \Large{Benchmark Algorithm of Ensemblist Methods on Spatial Data.}}\\[0.2cm]
\rule{\linewidth}{0.4mm}\\[0.3cm]
\large{\textsc{Quarto Report}}
 
\vskip 80pt
 
\begin{flushleft} \large
\emph{Students:}\\\vspace{0.2cm}
Andrea \textsc{Bianco} \\
Matteo \textsc{Mancini} \\
Rodrigue \textsc{Mellot}\\
\end{flushleft}
 
\begin{flushright} \large
\emph{Under the direction of :} \\ \vspace{0.2cm}
Olivier \textsc{Meslin} \\ \vspace{0.6cm}
\end{flushright}
 
\vfill
{\large Promotion  2026}
\end{center}
\end{titlepage}
\clearpage

# Abstract

Spatial interpolation is a fundamental challenge in data analysis, necessitating robust methods for estimating unknown values from scattered observations. This study provides a comprehensive benchmark to identify the most effective algorithms currently available. We begin with a literature review to classify existing techniques and analyze their theoretical advantages. To expand the scope of possible solutions, we also introduce and test ensemble learning models integrated with coordinate rotation alongside traditional methods. Our experimental framework employs a hierarchical evaluation strategy: initial screening on small datasets is used to discriminate and filter underperforming algorithms, followed by testing of the remaining candidates on complex, high-dimensional datasets. We compare these methods based on scalability, computational efficiency, and robustness. Ultimately, this study offers a definitive guide to the "state-of-the-art" in spatial interpolation, highlighting which algorithms perform best across varying levels of difficulty and data density.

# Introduction

Spatial interpolation is a fundamental technique within the environmental sciences, used extensively to estimate the values of continuous variables at unsampled locations based exclusively on spatial coordinates. For decades, deterministic and geostatistical methods have been regarded as the premier choice for these tasks due to their mathematical foundations in spatial continuity. However, the recent advancement of ensemble learning methods (such as Random Forest and Gradient Boosting) has begun to shift this perspective. These modern algorithms offer the potential to model complex, non-linear spatial surfaces that traditional methods may struggle to capture accurately.

The central research question of this benchmark involves identifying which algorithm is most effective for predicting spatial values using coordinates as the only covariates. In defining the most effective algorithm, this study looks beyond mere accuracy. We define the optimal model through the lens of both predictive precision and scalability. Precision measures the ability of a model to minimize error across diverse spatial geometries, while scalability evaluates the computational efficiency of the algorithm as it transitions from sparse datasets to large-scale high-density data.

Much of this research is motivated by the proposition that coordinate rotation can be integrated into ensemble methods to effectively replace or outperform traditional deterministic algorithms. While standard tree-based models often struggle with axis-aligned splits in spatial contexts, the coordinate rotation paradigm suggests that transforming the feature space can unlock significantly higher performance. One of the primary goals of this benchmark is to rigorously test this hypothesis and develop a clear understanding of the true capabilities and limitations of coordinate rotation in spatial modeling.

To achieve a robust evaluation, this study examines eight datasets comprising both real-world topographic data and synthetic stationary random fields. We compare a dozen distinct algorithms, ranging from k-nearest neighbors and kriging to advanced tree-based variants and generalized additive models. By analyzing these results across varied conditions, such as grid versus non-grid structures and small versus large sample sizes, this report seeks to provide a definitive comparison that guides practitioners in choosing the most efficient method for their specific spatial interpolation needs.

# Methodology

## General Framework

Overview of the benchmarking framework and experimental pipeline.

## Algorithms Considered

|                                                    |                                    |
|----------------------------------------------------|------------------------------------|
| A\) Generalized Additive Models - GAM ()           | F\) Nearest Neighbor Interpolation |
| B\) GeoSpatial Random Forest (Geerts et al. 2024a) | G\) Oblique Random Forest          |
| C\) Gradient Boosting                              | H\) Ordinary Kriging               |
| D\) Inverse Distance Weighting - IDW ()            | I\) Random Forest                  |
| E\) MI-GBT (Geerts et al. 2024b)                   | J\) XGBoost                        |

# Comparison of Spatial Interpolation Methods

Our benchmark compares a large range of spatial interpolation algorithms that can be grouped into three distinct families based on their theoretical foundations: deterministic methods, geostatistical methods, and machine learning approaches. Understanding these distinctions is essential for interpreting our results and providing practical guidance for model selection.

### Deterministic Methods

**Inverse Distance Weighting** (IDW) and **K-Nearest Neighbors** (K-NN) belong to the ***deterministic family*** of spatial interpolation methods. Given identical input data, they always produce the same output, as their formulation contains no stochastic component. Predictions are obtained through fixed mathematical rules that operate directly on spatial distances, rather than through an explicit data-driven learning mechanism.

However, it is important to remember that in this context *deterministic* does **not** mean *assumption-free*. In practice, both IDW and KNN embed **implicit assumptions** about the underlying spatial process through the choice of **hyperparameters**, which determine the effective spatial scale of smoothing and the degree of locality in the interpolation.

For **Inverse Distance Weighting**, the prediction at a target location $s_0$ is computed as a distance-weighted average of observed values: $$\hat{z}(s_0)=\frac{\sum_{i=1}^{n} w_i(s_0)\,z(s_i)}{\sum_{i=1}^{n} w_i(s_0)},\qquad w_i(s_0)=\frac{1}{d(s_0,s_i)^{p}},$$

where $d(s_0,s_i)$ denotes the Euclidean distance and $p>0$ is the **distance-decay exponent**. Larger values of $p$ impose a stronger locality assumption, causing nearby observations to dominate the prediction, while smaller values yield smoother surfaces by allowing distant points to contribute more substantially. Consequently, the choice of $p$ effectively encodes an assumption about how rapidly spatial influence should decay with distance.

For **K-Nearest Neighbors**, predictions are obtained by averaging the values of the $k$ closest observations: $$\hat{z}(s_0)=\frac{1}{k}\sum_{i\in \mathcal{N}_k(s_0)} z(s_i),$$where $\mathcal{N}_k(s_0)$ denotes the set of the $k$ nearest neighbors of $s_0$ (optionally using distance-based weights). Here, $k$ acts as a **bandwidth parameter**: small values of $k$ assume highly local spatial variation and may lead to noisy predictions, whereas larger values enforce stronger smoothing and implicitly assume a more slowly varying spatial surface.

Both methods are thus based purely on geometric principles and do not rely on any underlying statistical model. This simplicity offers clear practical advantages: they are easy to implement, as they only require basic distance computations, and they are computationally efficient because no training phase is needed. Their deterministic nature also makes them highly interpretable, since it is straightforward to understand how each prediction is obtained.

At the same time, this simplicity comes with limitations. These methods do not explicitly capture complex spatial structures that may be present in the data and rely on rigid monotonic distance assumptions. For instance, IDW presumes that the influence of observations decreases smoothly with distance, an assumption that may not hold in all spatial contexts. Despite these drawbacks, their speed and robustness make them valuable baseline approaches in spatial interpolation benchmarks.

For this reason, deterministic methods provide a useful baseline for spatial interpolation, against which more complex probabilistic and learning-based approaches can be meaningfully compared.

#### Geostatistical Method

*Kriging** represents the geostatistical approach to spatial interpolation. Unlike deterministic methods, it treats spatial data as realizations of an underlying stochastic process. This probabilistic framework allows the method to explicitly model spatial autocorrelation, determine prediction weights by minimizing the variance of the prediction error, and provide uncertainty estimates associated with the predictions.

At the core of the approach lies the variogram, which describes how similarity between observations changes with distance. Rather than simply assuming that nearby points are more influential, as deterministic methods do, Kriging asks how strong this influence is and how it evolves across spatial scales. Based on this information, optimal prediction weights are obtained by solving a system of linear equations that accounts for both inter-point distances and the overall spatial configuration.

There are several types of Kriging methods based on different assumptions about the spatial trend: **Universal Kriging** (polynomial drift with unknown coefficients), **Simple Kriging** (known constant mean), and **Ordinary Kriging**. In this benchmark, we focus on Ordinary Kriging.

Formally, the Ordinary Kriging predictor at an unobserved location $s_0$ is defined as a linear combination of the observed values, $\hat{Z}(s_0) = \sum_{i=1}^{n} \lambda_i \, Z(s_i)$, subject to the unbiasedness constraint $\sum_{i=1}^{n} \lambda_i = 1$. The weights $\lambda_i$ are chosen so as to minimize the prediction error variance $\operatorname{Var}\bigl(\hat{Z}(s_0) - Z(s_0)\bigr)$ under the assumed variogram model.

These theoretical foundations give Kriging several important advantages. Under its assumptions, Ordinary Kriging is the **Best Linear Unbiased Predictor (BLUP)**, meaning that among all predictors that are linear combinations of the data and unbiased, it minimizes the variance of the prediction error. It also guarantees exact interpolation at observed locations and produces smooth, continuous surfaces, avoiding the abrupt artifacts that may arise with simpler approaches. In addition, it naturally provides measures of predictive uncertainty, enabling the construction of confidence intervals.

However, these theoretical advantages come with substantial practical challenges. Kriging has **cubic computational complexity**, as it requires the inversion of an $n \times n$ covariance matrix, where $n$ is the number of observations. As a result, it becomes **prohibitively slow for datasets larger than roughly 10,000 points**, limiting its applicability in large-scale spatial problems.

In addition, the method relies on strong statistical assumptions. It **assumes** intrinsic **stationarity**, implying a constant mean over the study area and a variogram that depends only on the distance between points, not on their absolute location, **a condition that is often violated in real-world scenarios**. For example, in topographic elevation data, the mean altitude may vary systematically between valleys and mountain ranges; in urban environments, land values or pollution levels may differ markedly between city centers and suburban areas; and in climate-related applications, temperature or precipitation fields often exhibit large-scale spatial trends driven by latitude, altitude, or proximity to the sea. In such cases, assuming a constant mean across the domain can lead to biased predictions and distorted uncertainty estimates.

Kriging also assumes **isotropy**, meaning that spatial correlation depends solely on distance and not on direction. This assumption may be **unrealistic in the presence of directional effects**, such as prevailing wind patterns influencing air pollution dispersion, river networks imposing anisotropic dependence in hydrological variables, or geological strata generating directional continuity in subsurface properties. When such anisotropy is present but ignored, the variogram model becomes misspecified, potentially leading to degraded predictive performance.

Beyond these computational and theoretical issues, Kriging also presents practical implementation difficulties. Prior to prediction, the variogram must be estimated from the data. This step requires selecting a theoretical variogram model—such as spherical, exponential, or Gaussian—and estimating its key parameters, including the nugget, sill, and range. This procedure is often partly subjective, demands experience and judgment, and can be sensitive to outliers. Since prediction quality depends directly on the variogram specification, a poorly fitted model can result in unreliable estimates. Moreover, as a linear method, Kriging has limited ability to represent highly nonlinear spatial patterns and may overfit when applied to small datasets. It also implicitly assumes Gaussianity of the data, making transformations necessary when variables are strongly skewed.

Taken together, these limitations imply that although Kriging is theoretically optimal under its assumptions, it is not always the most practical choice in real-world settings. The gap between theoretical optimality and empirical performance is therefore a central theme in the benchmark results presented in this study.

These limitations motivate the exploration of alternative approaches that relax such assumptions while retaining strong predictive performance.


#### Machine Learning Methods

The machine learning (ML) approaches considered in this benchmark represent a fundamentally different paradigm from both deterministic and geostatistical methods. Rather than relying on fixed mathematical rules or explicit stochastic models of spatial dependence, ML algorithms are **data-driven** and learn predictive relationships directly from the data by optimizing a loss function. This property makes them highly flexible, scalable, and particularly effective in high-dimensional settings.

Within the ML family, we distinguish **two main classes of methods**: **tree-based ensemble methods** and **Generalized Additive Models (GAM)**. This distinction is important, as it reflects different modeling philosophies and leads to markedly different behaviors when applied to spatial data.


##### Tree-based Ensemble Methods

Tree-based ensemble methods are among the most widely used machine learning algorithms for tabular data and are known for their **state-of-the-art predictive performance**, **robustness, and scalability**. In this benchmark, we focus on two major subfamilies: **Random Forest (RF)** and **Gradient Boosting (GB)** methods.

Random Forest is based on a **bagging strategy**, where multiple decision trees are trained on **bootstrap samples** of the data and their predictions are averaged. Each tree partitions the feature space through **recursive binary splits** designed to **minimize prediction error within each region**. The aggregation of many decorrelated trees results in stable predictions with reduced variance.

Gradient Boosting methods, such as **XGBoost** and **MI-GBT**, adopt a different strategy. Trees are built **sequentially**, with each new tree trained to correct the residual errors of the existing ensemble. This iterative refinement **often** yields **higher predictive accuracy** than bagging-based methods, at the cost of increased **sensitivity to hyperparameter tuning** and a **higher risk of overfitting** if not properly regularized.

A key advantage of tree-based methods lies in their broad **applicability across a wide range of prediction tasks**, as they can be employed **without relying on strong assumptions** about the underlying data-generating process. Moreover, they are highly extensible, allowing additional covariates to be seamlessly incorporated into the model. For instance, in applications such as housing price prediction, spatial coordinates can be combined with socioeconomic or structural variables, a level of flexibility that is difficult to achieve with traditional geostatistical approaches.

#### Generalized Additive Models

Generalized Additive Models represent a distinct class within the ML family. GAMs model the target variable as a **sum of smooth nonlinear functions of the input features**, balancing flexibility and interpretability. While not tree-based, GAMs can capture nonlinear spatial trends through smooth functions of the coordinates, positioning them at the boundary between classical statistical modeling and modern machine learning. Their additive structure makes them more interpretable than ensemble methods, but also limits their ability to represent highly complex spatial interactions.

#### Adapting Tree-based Methods to Geospatial Data

Despite their strong performance in many domains, tree-based methods are not specifically designed for geospatial data. When applied naively to spatial coordinates, they face several well-known challenges inherent to spatial processes, including spatial autocorrelation, anisotropy, and the presence of large-scale spatial trends.

Spatial autocorrelation refers to the tendency of nearby observations to exhibit similar values. For example, environmental variables such as air pollution or temperature often display strong local correlation, meaning that observations are highly dependent on their neighbors. Standard tree-based models do not explicitly account for this dependence structure and may therefore overfit local noise. Similarly, many spatial phenomena display anisotropy, where dependence varies with direction rather than distance alone, as in the case of pollutant dispersion driven by prevailing winds or hydrological variables constrained by river networks. Axis-aligned decision trees struggle to capture such directional patterns efficiently, often requiring deep trees and many splits to approximate oblique spatial gradients.

Large-scale spatial trends present a particularly important challenge for tree-based methods. These trends represent smooth, gradual variations that span the entire spatial domain, such as elevation decreasing from mountainous regions to lowlands, temperature declining with latitude or altitude, or housing prices increasing radially from urban centers. Tree-based models approximate such trends using axis-aligned rectangular partitions, which is inherently inefficient. Capturing a smooth diagonal or curved trend requires many stepwise splits, leading to deep trees, high model complexity, and potential overfitting to local variations while failing to capture the global pattern effectively.

As a consequence, achieving optimal performance with tree-based methods in geospatial settings typically requires adaptation. Broadly speaking, there are two alternative strategies to address these challenges. The first strategy consists in modifying the algorithms themselves to make them more suitable for spatial data. Examples include Geographic Random Forest, which explicitly incorporates spatial information into the ensemble construction, and Oblique Random Forest, which allows decision boundaries that are not constrained to be parallel to the coordinate axes. By enabling oblique splits, these methods can more naturally represent spatial gradients that are misaligned with the original coordinate system.

The second strategy focuses on preprocessing the spatial data in a way that makes it better suited to standard tree-based algorithms, without altering their internal functioning. A prominent example of this approach is coordinate rotation, which augments the feature space with rotated versions of the original spatial coordinates. This transformation allows axis-aligned trees to effectively learn oblique spatial patterns, mitigating the geometric limitations of standard decision trees while preserving their computational efficiency and scalability.

In this benchmark, we investigate both adaptation strategies, with particular emphasis on coordinate rotation as a simple yet powerful preprocessing technique.

## Coordinate Rotation

### Addressing Anisotropy: Oblique Trees versus Coordinate Rotation

Regarding spatial interpolation, all standard tree-based ensemble methods share a fundamental limitation: they rely exclusively on axis-aligned splits during tree construction. At each node, the algorithm selects a single feature and a threshold value to partition the data along that coordinate axis. This implies that when dealing with spatial data, tree-based models produce geographical splits with either a North-South (vertical) orientation or a (horizontal) East-West orientation. Obviously, spatial phenomena may have any spatial orientation, implying that off-the-shelf tree-based models must approximate diagonal spatial patterns by an accumulation of horizontal or vertical splits, leading to a staircase approximation.

To overcome these limitations, two main strategies can be considered. The first one is represented by Oblique Decision Trees, an algorithms that directly learn oblique splits by constructing linear combinations of features at each node; however, these methods are computationally demanding. The second strategy is our Coordinate Rotation, a simpler approach that augments the feature space with rotated versions of the original coordinates, allowing standard axis-aligned algorithms to effectively learn oblique boundaries.

The Coordinate rotation method is based on the transformation of the original spatial coordinates $(x, y)$ by applying a series of rotation transformation around the centroid of the training data. Using this procedure we are able to obtain an icreased number of feature space containing multiple and different representations of the same space locations, unique for their rotation angle. The precise procedure is detailed in the following pseudocode.

\begin{algorithm}
\caption{Spatial Feature Augmentation by Coordinate Rotation}
\begin{algorithmic}[1]
\Require Training dataset $\{(x_i, y_i)\}_{i=1}^n$, number of axes $k$
\Ensure Augmented feature matrix with original and rotated coordinates

\State Compute centroid:
\Statex \quad $\bar{x} \gets \frac{1}{n} \sum_{i=1}^n x_i$ \quad $\bar{y} \gets \frac{1}{n} \sum_{i=1}^n y_i$

\State Generate rotation angles and apply rotation:
\For{$j = 0$ to $k-1$}
    \State $\theta_j \gets \frac{360^\circ \cdot j}{k}$
    \For{$i = 1$ to $n$}
        \State $x'_{ij} \gets \bar{x} + (x_i - \bar{x}) \cos(\theta_j) - (y_i - \bar{y}) \sin(\theta_j)$
        \State $y'_{ij} \gets \bar{y} + (x_i - \bar{x}) \sin(\theta_j) + (y_i - \bar{y}) \cos(\theta_j)$
    \EndFor
\EndFor

\State Return augmented features: $\{(x'_{i0}, y'_{i0}, x'_{i1}, y'_{i1}, \ldots, x'_{i,k-1}, y'_{i,k-1})\}_{i=1}^n$

\end{algorithmic}
\end{algorithm}



### Model Selection for Coordinate Rotation

Not all algorithms benefit equally from coordinate rotation. For this reason, we apply the technique selectively based on each algorithmic properties:

**Models with Coordinate Rotation Variants**: 
- Random Forest (RF vs RF-CR) - XGBoost (XGB vs XGB-CR) - MI-GBT (MI-GBT vs MI-GBT-CR) - GAM (GAM vs GAM-CR)

The remaining algorithm are used only for a comparative purpose. This allow us to see how the coordinate rotations based models perform with respect to other existing strategy and isolate the specific impact of coordinate rotation on standard axis-aligned tree algorithms. 

### Geometric Interpretation

From a geometric perspective, coordinate rotation can be interpreted as a change of reference system applied to the spatial domain. Each rotation defines a new coordinate system whose axes are oriented at a specific angle with respect to the original one. Within these rotated systems, spatial patterns that appear oblique in the original coordinates may become aligned with the axes, allowing tree-based models to represent them through simple axis-aligned splits.

# Experimental Setup and Evaluation Metrics

## Datasets

There will be plenty datasets to cover combinations of:

-   **Real or Synthetic**
-   **Large or Small**
-   **Grid or No Grid**

It will also be about, very large, extremly large datasets and noisy datasets. 

For the **synthetic ones**, they are built following the idea that we need some data spatially correlated, be able to respect our different criteria:

-   **Spatial Correlation**: We use a **Matérn covariance model** (dimension=2, variance=1, length scale=10) to generate a Stationary Random Field (SRF). This ensures the synthetic data mimics the spatial continuity and "smoothness" often found in real-world environmental phenomena.
-   **Structure**: If there is a grid, points are generated on a regular Cartesian grid using a meshgrid of and coordinates. If it's not the case, points are sampled using a\*Uniform Random Distribution across the spatial domain to simulate irregular sampling.
-   **Size**: Ranging from 5,000 points for "Small" Datasets, 100,000 points for "Large" datasets, 1,000,000 points for "Very Large" datasets and 10,000,000 points for "Extrem Large"
-   **Consistency**: A fixed seed (20170519) is applied to both the random field generation and the coordinate sampling to ensure the experiments are fully reproducible across different benchmark runs.

**Noisy**: For the noisy datasets, they are build using gaussian noisy, with a variance equal to 0,5 for the low noise, 1 for medium noise and two for high noise.

For the **real datasets**, we utilize high-quality topographic data provided by the **IGN (Institut National de l'Information Géographique et Forestière)**, the French national mapping agency.

-   **BD ALTI**: This dataset represents the "unstructured" real-world scenario. The points are derived from various sources (photogrammetry, digitization, etc.) where the spatial distribution of samples is irregular. So we can use this dataset as our no grid, large, real dataset.
-   **RGE ALTI**: It is the highest resolution elevation model available nationally. It is provided as a 5-meter regular grid. The full national dataset contains over 22 billion points. So we can use this dataset as our grid, large, real dataset.

- **California Housing**: A standard machine learning benchmark derived from the 1990 U.S. Census. We include this dataset to test model robustness in a "medium-sized, noisy, real-world" scenario, contrasting with the relatively smooth topographic surfaces of the IGN data.

For the Small real-world datasets, we use a subset of the French territory by filtering for Department 48 (Lozère). This department was chosen because its diverse topography—ranging from deep canyons and plateaus to mountainous terrain—offers a representative sample of various geographic challenges for spatial interpolation.

## Dataset Reference Table

| Dataset Name | Origin | Size Category | Structure | Notes / Noise Level | Approx. Row Count |
| --- | --- | --- | --- | --- | --- |
| **bdalti** | Real | Large | No Grid | Clean | ~7,000,000 |
| **bdalti_48** | Real | Small | No Grid | Clean | ~40,000 |
| **rgealti** | Real | Large | Grid | Clean | ~22,000,000,000 |
| **rgealti_48** | Real | Small | Grid | Clean | ~1,500,000 |
| **S-G-Sm** | Synthetic | Small | Grid | Clean | 5,000 |
| **S-G-Lg** | Synthetic | Large | Grid | Clean | 100,000 |
| **S-NG-Sm** | Synthetic | Small | No Grid | Clean | 5,000 |
| **S-NG-Lg** | Synthetic | Large | No Grid | Clean | 100,000 |
| **S-NG-VLg** | Synthetic | Very Large | No Grid | Clean | 1,000,000 |
| **S-NG-ELg** | Synthetic | Extremely Large | No Grid | Clean | 10,000,000 |
| **S-NG-Lg-N1** | Synthetic | Large | No Grid | Low Noise | 100,000 |
| **S-NG-Lg-N2** | Synthetic | Large | No Grid | Med Noise | 100,000 |
| **S-NG-Lg-N3** | Synthetic | Large | No Grid | High Noise | 100,000 |
| **cal_housing** | Real | Medium | No Grid | Noisy (Real World) | ~20,000 |


## Performance Metrics

We evaluate all models using three complementary metrics computed on the held-out test set:

-   **R² (Coefficient of Determination)**: Measures the proportion of variance in the target variable explained by the model. This metric is scale-invariant and provides an intuitive measure of overall model fit.

-   **RMSE (Root Mean Squared Error)**: Quantifies prediction error in the original units of the target variable. Penalizes large errors more heavily than small ones due to the squaring operation. Lower values indicate better performance.

-   **MAE (Mean Absolute Error)**: Measures average absolute prediction error in original units. More robust to outliers than RMSE. Lower values indicate better performance.

-   **Training Time**: Wall-clock time (in seconds) required for model fitting on the training set. Provides insight into computational efficiency and practical scalability.



## Benchmark Procedure   

All experiments ran in a Python 3.13 environment on SSPCloud with full parallelization (n_jobs=-1). To ensure reproducibility, a fixed random seed of 42 was applied across all generators. Data preprocessing included log-transforming skewed elevation targets and applying a 23-angle Coordinate Rotation (CR) for tree-based models to mitigate orthogonal split biases.

Our validation strategy employs a robust Randomized Search with K-Fold Cross-Validation. The procedure iterates through datasets and algorithms (a double loop), randomly sampling hyperparameter spaces. Each configuration is evaluated across K folds, with the "Best Model" selected based on the lowest average RMSE. We adapted tuning intensity to dataset size: high-intensity search (20 iterations, 5-fold, 10-min timeout) for small data, and an efficiency-focused regime (5 iterations, 3-fold) for large data.


# Results

The study follows a two-phase selection process. First, we benchmark all algorithm families on small datasets (N≈5,000) to map the initial tradeoffs between precision and computational cost. Based on these results, we preselect the most efficient candidates to undergo further tests on larger datasets and noisy scenarios, assessing their true scalability and robustness in different conditions.

```{python}
#| echo: false
import json
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from pathlib import Path
from matplotlib.lines import Line2D
from matplotlib.patches import Patch

# ============================================================================
# CONFIGURATION
# ============================================================================

sns.set_style("white") 
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10

# --- 1. Load and Merge Data ---
# We try to load both small and large/noisy result files
results = []

def load_json_results(filename):
    path = Path(filename)
    if path.exists():
        with open(path, 'r') as f:
            data = json.load(f)
            # Support both list-of-dicts or dict-with-results-key format
            if isinstance(data, list): return data
            if isinstance(data, dict) and 'results' in data: return data['results']
    return []

# Load both files generated by the benchmark scripts
results.extend(load_json_results('results/results_small.json'))
results.extend(load_json_results('results/results_large_noisy.json'))

# Fallback for testing (remove in prod)
if not results and Path('example.json').exists():
    with open('example.json', 'r') as f:
        results = json.load(f)['results']

# --- 2. Definitions ---

# Dataset Keys
datasets = [
    'rgealti_48', 'rgealti', 'bdalti_48', 'bdalti', 
    'S-G-Sm', 'S-G-Lg', 'S-NG-Sm', 'S-NG-Lg',
    'S-G-Lg-N', 'S-NG-Lg-N', 'cal_housing'
]

# Model Groups
deterministic_models = ['knn_3', 'idw_p3']
geostatistical_models = ['kriging']
tree_models = ['random_forest', 'random_forest_cr', 'xgboost', 'xgboost_cr', 'mixgboost', 'mixgboost_cr']
advanced_tree_models = ['oblique_rf', 'geoRF']
gam_models = ['gam', 'gam_cr']

model_names = {
    'random_forest': 'RF', 'random_forest_cr': 'RF-CR',
    'xgboost': 'XGB', 'xgboost_cr': 'XGB-CR',
    'mixgboost': 'MiGBT', 'mixgboost_cr': 'MiGBT-CR',
    'oblique_rf': 'Oblique RF', 'geoRF': 'GeoRF',
    'knn_3': 'KNN-3', 'idw_p3': 'IDW-p3',
    'kriging': 'Kriging', 'gam': 'GAM', 'gam_cr': 'GAM-CR'
}

dataset_names = {
    'rgealti_48': 'Real Grid Small',
    'rgealti': 'Real Grid Large',
    'bdalti_48': 'Real No-Grid Small',
    'bdalti': 'Real No-Grid Large',
    'S-G-Sm': 'Synth Grid Small',
    'S-G-Lg': 'Synth Grid Large',
    'S-NG-Sm': 'Synth No-Grid Small',
    'S-NG-Lg': 'Synth No-Grid Large',
    'S-G-Lg-N': 'Synth Grid Large (Noisy)',
    'S-NG-Lg-N': 'Synth No-Grid Large (Noisy)',
    'cal_housing': 'Housing (Real Noisy)'
}

color_deterministic = '#2E86AB'
color_geostat = '#A23B72'
color_tree = '#F18F01'
color_advanced_tree = '#C73E1D'
color_gam = '#6A994E'

def get_model_color(model):
    if model in deterministic_models: return color_deterministic
    if model in geostatistical_models: return color_geostat
    if model in tree_models: return color_tree
    if model in advanced_tree_models: return color_advanced_tree
    if model in gam_models: return color_gam
    return '#888888'
```

## Small Dataset Screening: Computational Efficiency vs Accuracy

As mentioned previously, we begin our analysis by running all candidate algorithms on Small datasets. This step allows us to evaluate the tradeoff between **Training Cost (Time)** and **Predictive Accuracy (RMSE)** before scaling up.

The figure below plots the Training Time (x-axis, log scale) against the RMSE (y-axis). The ideal algorithm would be located in the **bottom-left corner** (fast and accurate).

```{python}
#| echo: false
from matplotlib.lines import Line2D

def plot_small_datasets_landscape():
    """
    Plot: Training Time (X) vs RMSE (Y) for Small Datasets.
    Color: Algorithm
    Shape: Dataset
    """
    small_ds = ['rgealti_48', 'bdalti_48', 'S-G-Sm', 'S-NG-Sm']
    
    ds_shapes = {
        'rgealti_48': 'o', 'bdalti_48': 's',
        'S-G-Sm': '^', 'S-NG-Sm': 'D'
    }
    
    plot_data = []
    
    for m_key in model_names.keys():
        for ds in small_ds:
            res = next((r for r in results if r['model'] == m_key and r['dataset'] == ds), None)
            if res:
                val_rmse = res.get('rmse', np.sqrt(res.get('mse', 0)))
                
                plot_data.append({
                    'Model': model_names[m_key],
                    'ModelKey': m_key,
                    'Dataset': dataset_names.get(ds, ds), # Safe get
                    'DatasetKey': ds,
                    'Time': res.get('training_time', 0.001),
                    'RMSE': val_rmse,
                    'Color': get_model_color(m_key),
                    'Shape': ds_shapes[ds]
                })

    df = pd.DataFrame(plot_data)
    
    fig, ax = plt.subplots(figsize=(10, 7))
    
    for _, row in df.iterrows():
        ax.scatter(row['Time'], row['RMSE'], 
                   c=row['Color'], marker=row['Shape'], 
                   s=100, alpha=0.7, edgecolors='black', linewidth=0.5)

    ax.set_xscale('log')
    ax.set_xlabel('Training Time (seconds) - Log Scale')
    ax.set_ylabel('RMSE (Lower is Better)')
    ax.set_title('Small Datasets: Efficiency vs. Accuracy Trade-off')
    ax.grid(True, linestyle='--', alpha=0.3)

    # --- LEGEND 1: Colors (Algorithms) ---
    legend_elements_color = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_deterministic, markersize=10, label='Deterministic'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_tree, markersize=10, label='Tree Ensemble'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_advanced_tree, markersize=10, label='Advanced Tree'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_geostat, markersize=10, label='Geostatistical'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_gam, markersize=10, label='GAM'),
    ]
    # Save this legend to a variable
    first_legend = ax.legend(handles=legend_elements_color, title="Algorithm Family", loc='upper left')
    
    # Crucial Step: Manually add the first legend back to the plot layout
    ax.add_artist(first_legend)

    # --- LEGEND 2: Shapes (Datasets) ---
    # We use gray markers to indicate "Shape only"
    legend_elements_shape = [
        Line2D([0], [0], marker=ds_shapes[ds], color='w', markerfacecolor='gray', 
               markeredgecolor='black', markersize=10, label=dataset_names.get(ds, ds))
        for ds in small_ds
    ]
    
    # Add the second legend to the bottom right
    ax.legend(handles=legend_elements_shape, title="Dataset Source", loc='lower right')
    
    plt.tight_layout()
    plt.show()

plot_small_datasets_landscape()

```

The plot reveals different computational profiles for the families of algorithms. Using the results obtained, a selection of models is made for evaluation on larger datasets. This is a crucial step that avoids wasting computational resources on models that are not competitive in terms of training time.

The GAM, GeoRF exhibit extreme training times even on modest sample sizes. Extrapolating these computational demands to datasets an order of magnitude larger suggests prohibitive resource requirements that would render them impractical for operational use. For the case of Kriging, its complexity leads to training times that grow rapidly with sample size, making it infeasible for larger datasets. These methods are therefore excluded from further analysis, as their computational inefficiency outweighs any potential accuracy benefits they may offer.

The remaining candidates demonstrate more favorable computational profiles. Ensemble methods cluster in the efficient region of the accuracy-time tradeoff space, delivering low prediction error with manageable training costs. Deterministic methods also remain in the analysis due to their near instantaneous training and competitive predictive performance. Their simplicity establishes a demanding baseline: any machine learning enhancement must deliver substantial accuracy gains to justify increased algorithmic complexity.

## Large-Scale Performance Evaluation

### Scalability Analysis

In the following step the scalability of the algorithms is evaluated using as comparison metric the **Augmentation Factor** defined as the ratio of time for large dataset over the time for small dataset. This metrics depends on the algorithm's parameter and specific dataset, so it is not rigorous from a statistical point of view, but it provides a useful measure of how well the training time scales as the dataset size increases. 
riscrivere che augmentation e 20X 

```{python}
#| echo: false
import pandas as pd
import numpy as np
from IPython.display import display

def display_scalability_table_colored():
    # --- Configuration ---
    small_ds = ['rgealti_48', 'bdalti_48', 'S-G-Sm', 'S-NG-Sm']
    large_ds = ['rgealti', 'bdalti', 'S-G-Lg', 'S-NG-Lg']
    excluded_models = ['geoRF', 'gam', 'gam_cr', 'kriging']
    
    table_data = []
    
    # --- Data Collection ---
    for m_key, m_name in model_names.items():
        if m_key in excluded_models:
            continue
            
        # Get color using your function
        row_color = get_model_color(m_key)
            
        times_sm = [r.get('training_time', 0) for r in results 
                   if r['model'] == m_key and r['dataset'] in small_ds]
        times_lg = [r.get('training_time', 0) for r in results 
                   if r['model'] == m_key and r['dataset'] in large_ds]
        
        avg_sm = np.mean(times_sm) if times_sm else 0
        avg_lg = np.mean(times_lg) if times_lg else 0
        
        if avg_sm > 0 and avg_lg > 0:
            factor = avg_lg / avg_sm
            factor_str = f"{factor:.1f}x"
        else:
            factor = 0
            factor_str = "-"

        table_data.append({
            'Model': m_name,
            'Avg Time (Small)': f"{avg_sm:.3f}s",
            'Avg Time (Large)': f"{avg_lg:.2f}s",
            'Augmentation Factor': factor_str,
            '_sort_key': factor,
            '_row_color': row_color  # Store color for styling
        })
    
    df = pd.DataFrame(table_data)
    
    # --- Styling Function ---
    def color_rows(row):
        # Apply the stored color to all visible columns
        color = row['_row_color']
        return [f'color: {color}; font-weight: bold' for _ in row]

    # --- Display ---
    if not df.empty:
        df_sorted = df.sort_values('_sort_key')
        
        # Apply style
        styled_df = (df_sorted.style
                     .apply(color_rows, axis=1)          # Apply color logic
                     .hide(axis="index")                 # Hide Index
                     .hide(subset=['_sort_key', '_row_color'], axis=1)) # Hide helper columns
        
        display(styled_df)

display_scalability_table_colored()
```

**Scaling Behavior Analysis:**

The analysis on larger datasets shows different behaviour for the considered algortihms. **Deterministic methods** exhibit one of the highest augmentation factor both for the KNN and for IDW, however this not implies a bad scalability because it still remains the fastest one. Also the **Tree ensemble methods** are able to scale well, with a reasonable augmentation factors. The XGBoost based model shows a particular pattern, infact, its training time for the large datasets is lower than one of the small datasets.

Adesso andiamo a comparare i risultati per dataset piu grandi (grandissimi)

```{python}
#| echo: false
import pandas as pd
from IPython.display import display

def display_stress_test_table_colored():
    # --- Configuration ---
    target_datasets = ['S-NG-Lg', 'S-NG-VLg', 'S-NG-ELg']
    excluded_models = ['geoRF', 'gam', 'gam_cr', 'kriging']
    
    table_data = []
    
    # --- Data Collection ---
    for m_key, m_name in model_names.items():
        if m_key in excluded_models:
            continue
            
        # Get color
        row_color = get_model_color(m_key)

        row = {
            'Model': m_name,
            '_row_color': row_color
        }
        
        # Helper to find time
        def get_time(d_name):
            val = next((r.get('training_time', 0) for r in results 
                        if r['model'] == m_key and r['dataset'] == d_name), None)
            return val

        t_lg = get_time('S-NG-Lg')
        t_vlg = get_time('S-NG-VLg')
        t_elg = get_time('S-NG-ELg')

        row['Time (S-NG-Lg)']  = f"{t_lg:.2f}s" if t_lg else "-"
        row['Time (S-NG-VLg)'] = f"{t_vlg:.2f}s" if t_vlg else "-"
        row['Time (S-NG-ELg)'] = f"{t_elg:.2f}s" if t_elg else "-"
        
        # Sort key
        row['_sort_key'] = t_lg if t_lg else 99999
        
        table_data.append(row)

    df = pd.DataFrame(table_data)
    
    # --- Styling Function ---
    def color_rows(row):
        color = row['_row_color']
        return [f'color: {color}; font-weight: bold' for _ in row]

    # --- Display ---
    if not df.empty:
        df_sorted = df.sort_values('_sort_key')
        
        styled_df = (df_sorted.style
                     .apply(color_rows, axis=1)
                     .hide(axis="index")
                     .hide(subset=['_sort_key', '_row_color'], axis=1))
        
        display(styled_df)

display_stress_test_table_colored()
```

### Robustness to Noise

The previous data used, whether real or synthetic, was relatively smooth. However, real-world scenarios often involve noisy data, such as housing prices, where variability cannot be perfectly modeled by coordinates alone. To test the models' resilience, we evaluated them on **three large synthetic datasets with increasing noise levels** (`S-NG-Lg-N1`, `N2`, `N3`) and one real noisy dataset: **California Housing**, trying to highlight the impact of coordinate rotation on RMSE in these conditions.


```{python}
#| echo: false
import s3fs
import json
import pandas as pd
from IPython.display import display, Markdown

def create_table_noisy_comparison():
    # 1. LOAD RESULTS
    s3_path = 's3://projet-benchmark-spatial-interpolation/results/results_large_noisy.json'
    fs = s3fs.S3FileSystem(anon=False)

    try:
        with fs.open(s3_path, 'r') as f:
            results = json.load(f)
    except Exception as e:
        print(f"Error loading S3 file: {e}")
        return pd.DataFrame()

    # 2. DEFINE TARGETS
    clean_ds = 'S-NG-Lg'
    noise_levels = ['S-NG-Lg-N1', 'S-NG-Lg-N2', 'S-NG-Lg-N3']
    housing_ds = 'cal_housing'

    # 3. BUILD TABLE
    table_data = []
    
    model_names_map = {
        'knn_3': 'KNN-3',
        'random_forest': 'Random Forest',
        'random_forest_cr': 'Random Forest + CR',
        'xgboost': 'XGBoost',
        'xgboost_cr': 'XGBoost + CR',
        'mixgboost': 'MiGBT',
        'mixgboost_cr': 'MiGBT + CR',
        'oblique_rf': 'Oblique RF',
        'idw_p3': 'IDW-p3',
    }
    
    key_models = ['knn_3', 'idw_p3', 'oblique_rf' ,'random_forest', 'random_forest_cr', 'xgboost', 'xgboost_cr', 'mixgboost','mixgboost_cr']

    for m_key in key_models:
        # Inner function defined here to capture 'm_key' from the loop scope
        def get_rmse(dataset_name):
            found = [r for r in results if r['model'] == m_key and r['dataset'] == dataset_name]
            return found[0]['rmse'] if found else None

        v_clean = get_rmse(clean_ds)
        v_n1 = get_rmse(noise_levels[0])
        v_n2 = get_rmse(noise_levels[1])
        v_n3 = get_rmse(noise_levels[2])
        v_housing = get_rmse(housing_ds)
        
        degradation_str = "-"
        
        if v_clean and v_n3:
            deg = ((v_n3 - v_clean) / v_clean) * 100
            degradation_str = f"+{deg:.0f}%"

        table_data.append({
            'Model': model_names_map.get(m_key, m_key),
            'RMSE (Clean)': f"{v_clean:.3f}" if v_clean else "N/A",
            'RMSE (N1)': f"{v_n1:.3f}" if v_n1 else "-",
            'RMSE (N2)': f"{v_n2:.3f}" if v_n2 else "-",
            'RMSE (N3)': f"{v_n3:.3f}" if v_n3 else "-",
            'Max Degr.': degradation_str,
            'Housing RMSE': f"{v_housing:.3f}" if v_housing else "-"
        })

    df = pd.DataFrame(table_data)

    # 4. DISPLAY WITHOUT INDEX
    if df.empty:
        return Markdown("### No data available to display.")
    else:
        # Using style.hide() removes the index column from the HTML representation
        return df.style.hide(axis='index')

# Display the result
display(create_table_noisy_comparison())
```

**Noise Robustness:**

A primary observation is that while the integration of CR consistently enhances model accuracy in low-noise environments—notably yielding a substantial reduction in **Root Mean Square Error (RMSE)** for both Random Forest (RF) and XGBoost—this advantage tends to diminish as the signal-to-noise ratio decreases. As noise levels intensify, the performance gap between standard and CR-augmented versions converges. In certain high-noise scenarios, the CR variants even exhibit a marginal increase in error, suggesting a practical threshold for the technique. From a computational efficiency standpoint, this indicates that the overhead of applying CR may not be justified when data quality is significantly compromised, as the incremental gains in precision no longer offset the increased processing time.

Furthermore, the data reveals a critical shift in model hierarchy as environmental complexity grows. While the **K-Nearest Neighbors (KNN)** algorithm demonstrates superior precision on clean datasets, it proves particularly susceptible to stochastic interference. This vulnerability is quantified by the **Maximum Degradation (Max Degr.)**, where KNN shows a significantly higher rate of error escalation compared to its ensemble-based counterparts. Such a trend points to a structural limitation inherent in deterministic models: their foundational assumptions of simplicity and local continuity struggle to capture underlying patterns once they are obscured by heavy noise. Finally, the analysis of the **California Housing** dataset assests with real-world examples.

### Impact of Coordinate Rotation

Having established computational feasibility and noise robustness patterns, attention turns to quantifying the specific contribution of coordinate rotation across diverse data conditions. The comparative evaluation considers high-volume datasets characterized by varying degrees of noise and structural complexity, consolidating observations from previous experiments into a coherent assessment framework. For each ensemble algorithm, performance is contrasted between standard implementations and their coordinate-rotated counterparts, examining both prediction error reduction and the associated computational burden required to achieve these gains.

```{python}
#| echo: false
import pandas as pd
import numpy as np
from IPython.display import display

def create_table_cr_manual_grouping():
    # 1. Config: Order and Abbreviations to save space
    large_ds_config = [
        ('rgealti', 'RGEALTI'), 
        ('bdalti', 'BDALTI'), 
        ('cal_housing', 'Housing'), 
        ('S-G-Lg', 'Syn-Grid'), 
        ('S-NG-Lg', 'Syn-NoGrid'), 
        ('S-NG-Lg-N1', 'Syn-NG-N1'), 
        ('S-NG-Lg-N2', 'Syn-NG-N2'), 
        ('S-NG-Lg-N3', 'Syn-NG-N3'), 
        ('S-NG-VLg', 'Syn-VeryLarge'), 
        ('S-NG-ELg', 'Syn-ExtremLarge')
    ]
    
    # Pairs: (Standard Key, CR Key, Short Display Name)
    tree_pairs = [
        ('random_forest', 'random_forest_cr', 'RF'), 
        ('xgboost', 'xgboost_cr', 'XGB'), 
        ('mixgboost', 'mixgboost_cr', 'MixGB')
    ]
    
    table_data = []

    # 2. Iterate and Collect Data
    for ds_key, ds_display in large_ds_config:
        is_first_row = True # Flag to handle the "Group Name"
        
        for std_key, cr_key, algo_name in tree_pairs:
            # Find results
            std_res = next((r for r in results if r['model'] == std_key and r['dataset'] == ds_key), None)
            cr_res = next((r for r in results if r['model'] == cr_key and r['dataset'] == ds_key), None)
            
            # Default values
            rmse_std_str, rmse_cr_str, imp_str = "-", "-", "-"
            time_std_str, time_cr_str, delay_str = "-", "-", "-"
            
            if std_res and cr_res:
                rmse_std = std_res.get('rmse')
                rmse_cr = cr_res.get('rmse')
                time_std = std_res.get('training_time', 0)
                time_cr = cr_res.get('training_time', 0)
                
                if rmse_std is not None and rmse_cr is not None:
                    # RMSE Logic
                    rmse_std_str = f"{rmse_std:.4f}"
                    rmse_cr_str = f"{rmse_cr:.4f}"
                    improvement = ((rmse_std - rmse_cr) / rmse_std) * 100
                    imp_str = f"{improvement:+.1f}%"
                    
                    # Time Logic
                    time_std_str = f"{time_std:.1f}"
                    time_cr_str = f"{time_cr:.1f}"
                    if time_std > 0:
                        delay = ((time_cr - time_std) / time_std) * 100
                        delay_str = f"+{delay:.0f}%"

            # 3. Handle the "Grouping" Visual
            # If it's the first algo (RF), show the Dataset Name. Otherwise, leave blank.
            row_label = ds_display if is_first_row else ""
            is_first_row = False 

            table_data.append({
                'Dataset': row_label,
                'Algo': algo_name,
                'RMSE (Std)': rmse_std_str,
                'RMSE (CR)': rmse_cr_str,
                'Imp.': imp_str,
                'T-Std(s)': time_std_str,  # Very short headers
                'T-CR(s)': time_cr_str,
                'Delay': delay_str
            })

    if not table_data:
        return pd.DataFrame({'Info': ['No matching data found.']})

    df = pd.DataFrame(table_data)
    
    # 4. Display with simple styling (Hide index, bold headers)
    # We strip the index to prevent the '0, 1, 2...' column
    return df.style.hide(axis='index').set_properties(**{
        'text-align': 'center',
        'vertical-align': 'middle'
    }).set_table_styles([
        dict(selector='th', props=[('text-align', 'center')]),
        # Add a bottom border to the last row of each group (every 3rd row)
        dict(selector='tbody tr:nth-child(3n)', props=[('border-bottom', '2px solid #ccc')])
    ])

display(create_table_cr_manual_grouping())
```


The results reveal a clear relationship between noise levels and the effectiveness of coordinate rotation. On structured datasets with low noise, CR delivers substantial improvements—RMSE reductions can exceed 60% on the RGEALTI dataset. However, these gains diminish progressively as noise increases. In highly noisy synthetic scenarios, the benefit becomes marginal or disappears entirely. Real-world datasets like California Housing show different behavior: when spatial autocorrelation dominates the signal, CR remains effective despite noise. This pattern indicates that CR is not a universal solution but works best when geographic structure genuinely drives the predictions rather than serving as a weak proxy for other factors.

The computational cost of coordinate rotation deserves attention. Training time increases substantially because the algorithm must process the expanded feature space. However, the absolute time penalty remains modest. Even with rotation, training completes within seconds rather than minutes for these datasets, maintaining practical feasibility. The augmented algorithms stay competitive with their standard counterparts in terms of wall-clock time, simply shifting from near-instantaneous to still-fast execution.

These observations suggest that coordinate rotation offers favorable tradeoffs for operational spatial modeling. The accuracy improvements justify the computational overhead when working with real world geographic data where spatial patterns matter. However, practitioners should assess their data characteristics before committing to the approach. When noise dominates the signal or coordinates correlate poorly with outcomes, the added complexity yields diminishing returns. The technique works when spatial structure exists; it cannot create predictive power where coordinates lack information.

## Discussion

The training time analysis confirms that deterministic models like IDW and KNN are nearly instantaneous, while advanced tree-based models and GAMs carry significantly higher computational burden. The scalability analysis shows that ensemble methods scale reasonably to large datasets, though their axis-aligned split logic creates artifacts on oblique spatial patterns without coordinate rotation. Performance across grid and non-grid structures reveals this geometric limitation clearly.

Overall, CR-enhanced ensembles achieve modestly lower RMSE than standard implementations , while deterministic methods maintain competitive performance without requiring coordinate augmentation. The practical significance of these gains depends on application context: in high-stakes prediction where marginal improvements matter, coordinate rotation offers a low-cost enhancement; for exploratory analysis or rapid prototyping, standard implementations suffice.

The central question was whether modern ensemble methods could replace traditional spatial interpolators. The answer is nuanced. Coordinate rotation enables tree-based methods to match or slightly exceed kriging on computational grounds  while achieving comparable prediction error on large datasets. However, the improvements over standard tree implementations are modest—typically 5-7% RMSE reduction—and come at the cost of feature engineering overhead.

Deterministic methods (KNN, IDW) remain highly competitive for rapid prototyping and applications where interpretability matters. They require no hyperparameter tuning, no training phase, and produce results instantly. Their prediction errors are only marginally higher than optimized ensemble methods in many scenarios. Therefore, the "best" algorithm depends critically on problem constraints: ensemble methods with coordinate rotation offer the highest accuracy when computational budget and implementation complexity permit; deterministic methods provide practical alternatives when simplicity, speed, or transparency dominate.

A significant limitation of this work is that the general datasets used may not capture the specific, high-frequency complex patterns where deterministic methods often excel due to their localized nature. Furthermore, the computational comparison is inherently biased because several algorithms rely on wraps for unmaintained or unoptimized packages, preventing them from reaching their true performance potential on our hardware. We are also limited by our reliance on  as the primary metric for the summary results. While  measures global variance, it can mask significant local errors that metrics like MAE or RMSE, which were calculated by the code but not fully explored in the main discussion, would reveal.


# Conclusion

The integration of **Coordinate Rotation (CR)** into ensemble methods allows for excellent results using algorithms that were not originally specialized for spatial interpolation tasks. This constitutes a promising discovery, especially given that these machine learning methods are continuously evolving and improving. However, to truly determine if they can consistently provide superior results, it is necessary to test them against datasets exhibiting more complex spatial patterns than those used in this general benchmark. Furthermore, while these ensemblist models are more time-consuming to compute compared to deterministic baselines, they prove to be highly robust as the dataset size increases. This scalability ensures they remain a viable and powerful option for high-density geographic modeling where traditional methods fail.

# References

Aalto, Juha, et al. "Spatial Interpolation of Monthly Climate Data for Finland: Comparing the Performance of Kriging and Generalized Additive Models." *Theoretical and Applied Climatology*, vol. 112, no. 1, 2013, pp. 99-111.

Anava, Oren, and Kfir Levy. "k*-Nearest Neighbors: From Global to Local." *Advances in Neural Information Processing Systems*, vol. 29, 2016.

Barry, Ronald Paul, and M. Jay Ver Hoef. "Blackbox Kriging: Spatial Prediction without Specifying Variogram Models." *Journal of Agricultural, Biological, and Environmental Statistics*, 1996, pp. 297-322.

Bentéjac, Candice, Anna Csörgő, and Gonzalo Martínez-Muñoz. "A Comparative Analysis of Gradient Boosting Algorithms." *Artificial Intelligence Review*, vol. 54, no. 3, 2021, pp. 1937-1967.

Borm, Conrad. "Kriging Models: An Extensive Analysis of Their Current State, Applicability, Advantages and Limitations." 2024. Utrecht University Student Theses, https://studenttheses.uu.nl/handle/20.500.12932/48040.

Bostan, Pınar. "Basic Kriging Methods in Geostatistics." *Yuzuncu Yıl University Journal of Agricultural Sciences*, vol. 27, no. 1, 2017, pp. 10-20.

Chen, Meifang, Changho Lee, and Yongwan Chun. "A Machine Learning Approach Using Spatially Explicit K-Nearest Neighbors for House Price Predictions." *ISPRS International Journal of Geo-Information*, vol. 15, no. 1, 2026, p. 46.

Ciampiconi, Lorenzo, et al. "A Survey and Taxonomy of Loss Functions in Machine Learning." *arXiv preprint arXiv:2301.05579*, 2023.

Cressie, Noel. *Statistics for Spatial Data*. John Wiley & Sons, 2015.

Dobson, Annette J., and Adrian G. Barnett. *An Introduction to Generalized Linear Models*. Chapman and Hall/CRC, 2018.

Gaudart, Jean, et al. "Oblique Decision Trees for Spatial Pattern Detection: Optimal Algorithm and Application to Malaria Risk." *BMC Medical Research Methodology*, vol. 5, no. 1, 2005, p. 22.

Geerts, Margot, Seppe Vanden Broucke, and Jochen De Weerdt. "A Spatial Loss Function for Gradient Boosted Trees." *CEUR Workshop Proceedings*, R. Piskac c/o Redaktion Sun SITE Informatik V RWTH Aachen, 2024.

Geerts, Margot, Seppe Vanden Broucke, and Jochen De Weerdt. "GeoRF: A Geospatial Random Forest." *Data Mining and Knowledge Discovery*, vol. 38, no. 6, 2024, pp. 3414-3448.

Hu, Hongda, and Hong Shu. "An Improved Coarse-Grained Parallel Algorithm for Computational Acceleration of Ordinary Kriging Interpolation." *Computers & Geosciences*, vol. 78, 2015, pp. 44-52.

Kern, Christoph, Thomas Klausch, and Frauke Kreuter. "Tree-Based Machine Learning Methods for Survey Research." *Survey Research Methods*, vol. 13, no. 1, 2019.

Lu, George Y., and David W. Wong. "An Adaptive Inverse-Distance Weighting Spatial Interpolation Technique." *Computers & Geosciences*, vol. 34, no. 9, 2008, pp. 1044-1055.

Nelder, John Ashworth, and Robert WM Wedderburn. "Generalized Linear Models." *Journal of the Royal Statistical Society Series A: Statistics in Society*, vol. 135, no. 3, 1972, pp. 370-384.

Oliver, M. A., and R. Webster. "A Tutorial Guide to Geostatistics: Computing and Modelling Variograms and Kriging." *Catena*, vol. 113, 2014, pp. 56-69.

Pilz, Jürgen, and Gunter Spöck. "Why Do We Need and How Should We Implement Bayesian Kriging Methods." *Stochastic Environmental Research and Risk Assessment*, vol. 22, no. 5, 2008, pp. 621-632.

Syam, Niladri, and Rajeeve Kaul. "Random Forest, Bagging, and Boosting of Decision Trees." *Machine Learning and Artificial Intelligence in Marketing and Sales: Essential Reference for Practitioners and Data Scientists*, Emerald Publishing Limited, 2021, pp. 139-182.

Wong, David WS. "Interpolation: Inverse-Distance Weighting." *International Encyclopedia of Geography: People, the Earth, Environment and Technology: People, the Earth, Environment and Technology*, 2016, pp. 1-7.

Wood, Simon N., Yannig Goude, and Simon Shaw. "Generalized Additive Models for Large Data Sets." *Journal of the Royal Statistical Society Series C: Applied Statistics*, vol. 64, no. 1, 2015, pp. 139-155.

