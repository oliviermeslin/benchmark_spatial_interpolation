---
# Keep metadata here for HTML/Internal use
title: "Benchmark Algorithm of Ensemblist Methods on Spatial Data"
author:
  - "Bianco Andrea"
  - "Mancini Matteo"
  - "Mellot Rodrigue"

format:
  pdf:
    documentclass: article
    papersize: a4
    title-block-render: false
    geometry:
      - top=20mm
      - bottom=20mm
      - left=20mm
      - right=20mm
    number-sections: true
    colorlinks: true
    fig-pos: "H"
    include-in-header:
      text: |
        \let\maketitle\relax 
    header-includes:
    - \usepackage{algorithm}
    - \usepackage{algpseudocode}
    - \usepackage{amsmath}

execute:
  enabled: true
  echo: false
  warning: false
  message: false
---

\begin{titlepage}
\begin{center}
\includegraphics[width=0.7\textwidth]{ensai_logo.png}\\[1.5 cm] 
 
\large{\textsc{Smart Data Project}}
\rule{\linewidth}{0.4mm} \\[0.3cm]
{\Large\bfseries \Large{Benchmark Algorithm of Ensemblist Methods on Spatial Data.}}\\[0.2cm]
\rule{\linewidth}{0.4mm}\\[0.3cm]
\large{\textsc{Quarto Report}}
 
\vskip 80pt
 
\begin{flushleft} \large
\emph{Students:}\\\vspace{0.2cm}
Andrea \textsc{Bianco} \\
Matteo \textsc{Mancini} \\
Rodrigue \textsc{Mellot}\\
\end{flushleft}
 
\begin{flushright} \large
\emph{Under the direction of :} \\ \vspace{0.2cm}
Olivier \textsc{Meslin} \\ \vspace{0.6cm}
\end{flushright}
 
\vfill
{\large Promotion  2026}
\end{center}
\end{titlepage}
\clearpage

# Abstract

Brief summary of the project, including: - the spatial interpolation problem, - the benchmarked methods, - the experimental design, - the main empirical results, - and the key conclusions.

# Introduction

Spatial interpolation is a fundamental technique within the environmental sciences, used extensively to estimate the values of continuous variables at unsampled locations based exclusively on spatial coordinates. For decades, deterministic and geostatistical methods have been regarded as the premier choice for these tasks due to their mathematical foundations in spatial continuity. However, the recent advancement of ensemble learning methods (such as Random Forest and Gradient Boosting) has begun to shift this perspective. These modern algorithms offer the potential to model complex, non-linear spatial surfaces that traditional methods may struggle to capture accurately.

The central research question of this benchmark involves identifying which algorithm is most effective for predicting spatial values using coordinates as the only covariates. In defining the most effective algorithm, this study looks beyond mere accuracy. We define the optimal model through the lens of both predictive precision and scalability. Precision measures the ability of a model to minimize error across diverse spatial geometries, while scalability evaluates the computational efficiency of the algorithm as it transitions from sparse datasets to large-scale high-density data.

Much of this research is motivated by the proposition that coordinate rotation can be integrated into ensemble methods to effectively replace or outperform traditional deterministic algorithms. While standard tree-based models often struggle with axis-aligned splits in spatial contexts, the coordinate rotation paradigm suggests that transforming the feature space can unlock significantly higher performance. One of the primary goals of this benchmark is to rigorously test this hypothesis and develop a clear understanding of the true capabilities and limitations of coordinate rotation in spatial modeling.

To achieve a robust evaluation, this study examines eight datasets comprising both real-world topographic data and synthetic stationary random fields. We compare a dozen distinct algorithms, ranging from k-nearest neighbors and kriging to advanced tree-based variants and generalized additive models. By analyzing these results across varied conditions, such as grid versus non-grid structures and small versus large sample sizes, this report seeks to provide a definitive comparison that guides practitioners in choosing the most efficient method for their specific spatial interpolation needs.

# Methodology

## General Framework

Overview of the benchmarking framework and experimental pipeline.

## Algorithms Considered

|                                       |                                    |
|----------------------------------------|--------------------------------|
| A\) Generalized Additive Models (GAM) | F\) Nearest Neighbor Interpolation |
| B\) GeoSpatial Random Forest          | G\) Oblique Random Forest          |
| C\) Gradient Boosting                 | H\) Ordinary Kriging               |
| D\) Inverse Distance Weighting (IDW)  | I\) Random Forest                  |
| E\) MI-GBT                            | J\) XGBoost                        |

# Comparison of Spatial Interpolation Methods

Our benchmark compares a large range of spatial interpolation algorithms that can be grouped into three distinct families based on their theoretical foundations: deterministic methods, geostatistical methods, and machine learning approaches. Understanding these distinctions is essential for interpreting our results and providing practical guidance for model selection.

### Deterministic Methods

**Inverse Distance Weighting** (IDW) and **K-Nearest Neighbors** (K-NN) belong to the ***deterministic family*** of spatial interpolation methods. Given identical input data, they always produce the same output, as their formulation contains no stochastic component. Predictions are obtained through fixed mathematical rules that operate directly on spatial distances, rather than through an explicit data-driven learning mechanism.

However, it is important to remember that in this context *deterministic* does **not** mean *assumption-free*. In practice, both IDW and KNN embed **implicit assumptions** about the underlying spatial process through the choice of **hyperparameters**, which determine the effective spatial scale of smoothing and the degree of locality in the interpolation.

For **Inverse Distance Weighting**, the prediction at a target location $s_0$ is computed as a distance-weighted average of observed values: $$\hat{z}(s_0)=\frac{\sum_{i=1}^{n} w_i(s_0)\,z(s_i)}{\sum_{i=1}^{n} w_i(s_0)},\qquad w_i(s_0)=\frac{1}{d(s_0,s_i)^{p}},$$

where $d(s_0,s_i)$ denotes the Euclidean distance and $p>0$ is the **distance-decay exponent**. Larger values of $p$ impose a stronger locality assumption, causing nearby observations to dominate the prediction, while smaller values yield smoother surfaces by allowing distant points to contribute more substantially. Consequently, the choice of $p$ effectively encodes an assumption about how rapidly spatial influence should decay with distance.

For **K-Nearest Neighbors**, predictions are obtained by averaging the values of the $k$ closest observations: $$\hat{z}(s_0)=\frac{1}{k}\sum_{i\in \mathcal{N}_k(s_0)} z(s_i),$$where $\mathcal{N}_k(s_0)$ denotes the set of the $k$ nearest neighbors of $s_0$ (optionally using distance-based weights). Here, $k$ acts as a **bandwidth parameter**: small values of $k$ assume highly local spatial variation and may lead to noisy predictions, whereas larger values enforce stronger smoothing and implicitly assume a more slowly varying spatial surface.

Both methods are thus based purely on geometric principles and do not rely on any underlying statistical model. This simplicity offers clear practical advantages: they are easy to implement, as they only require basic distance computations, and they are computationally efficient because no training phase is needed. Their deterministic nature also makes them highly interpretable, since it is straightforward to understand how each prediction is obtained.

At the same time, this simplicity comes with limitations. These methods do not explicitly capture complex spatial structures that may be present in the data and rely on rigid monotonic distance assumptions. For instance, IDW presumes that the influence of observations decreases smoothly with distance, an assumption that may not hold in all spatial contexts. Despite these drawbacks, their speed and robustness make them valuable baseline approaches in spatial interpolation benchmarks.

For this reason, deterministic methods provide a useful baseline for spatial interpolation, against which more complex probabilistic and learning-based approaches can be meaningfully compared.

#### Geostatistical Method

*Kriging** represents the geostatistical approach to spatial interpolation. Unlike deterministic methods, it treats spatial data as realizations of an underlying stochastic process. This probabilistic framework allows the method to explicitly model spatial autocorrelation, determine prediction weights by minimizing the variance of the prediction error, and provide uncertainty estimates associated with the predictions.

At the core of the approach lies the variogram, which describes how similarity between observations changes with distance. Rather than simply assuming that nearby points are more influential, as deterministic methods do, Kriging asks how strong this influence is and how it evolves across spatial scales. Based on this information, optimal prediction weights are obtained by solving a system of linear equations that accounts for both inter-point distances and the overall spatial configuration.

There are several types of Kriging methods based on different assumptions about the spatial trend: **Universal Kriging** (polynomial drift with unknown coefficients), **Simple Kriging** (known constant mean), and **Ordinary Kriging**. In this benchmark, we focus on Ordinary Kriging.

Formally, the Ordinary Kriging predictor at an unobserved location $s_0$ is defined as a linear combination of the observed values, $\hat{Z}(s_0) = \sum_{i=1}^{n} \lambda_i \, Z(s_i)$, subject to the unbiasedness constraint $\sum_{i=1}^{n} \lambda_i = 1$. The weights $\lambda_i$ are chosen so as to minimize the prediction error variance $\operatorname{Var}\bigl(\hat{Z}(s_0) - Z(s_0)\bigr)$ under the assumed variogram model.

These theoretical foundations give Kriging several important advantages. Under its assumptions, Ordinary Kriging is the **Best Linear Unbiased Predictor (BLUP)**, meaning that among all predictors that are linear combinations of the data and unbiased, it minimizes the variance of the prediction error **(Cressie, 1993; Chilès and Delfiner, 2012)**. It also guarantees exact interpolation at observed locations and produces smooth, continuous surfaces, avoiding the abrupt artifacts that may arise with simpler approaches. In addition, it naturally provides measures of predictive uncertainty, enabling the construction of confidence intervals.

However, these theoretical advantages come with substantial practical challenges. Kriging has **cubic computational complexity**, as it requires the inversion of an $n \times n$ covariance matrix, where $n$ is the number of observations. As a result, it becomes **prohibitively slow for datasets larger than roughly 10,000 points**, limiting its applicability in large-scale spatial problems.

In addition, the method relies on strong statistical assumptions. It **assumes** intrinsic **stationarity**, implying a constant mean over the study area and a variogram that depends only on the distance between points, not on their absolute location, **a condition that is often violated in real-world scenarios**. For example, in topographic elevation data, the mean altitude may vary systematically between valleys and mountain ranges; in urban environments, land values or pollution levels may differ markedly between city centers and suburban areas; and in climate-related applications, temperature or precipitation fields often exhibit large-scale spatial trends driven by latitude, altitude, or proximity to the sea. In such cases, assuming a constant mean across the domain can lead to biased predictions and distorted uncertainty estimates.

Kriging also assumes **isotropy**, meaning that spatial correlation depends solely on distance and not on direction. This assumption may be **unrealistic in the presence of directional effects**, such as prevailing wind patterns influencing air pollution dispersion, river networks imposing anisotropic dependence in hydrological variables, or geological strata generating directional continuity in subsurface properties. When such anisotropy is present but ignored, the variogram model becomes misspecified, potentially leading to degraded predictive performance.

Beyond these computational and theoretical issues, Kriging also presents practical implementation difficulties. Prior to prediction, the variogram must be estimated from the data. This step requires selecting a theoretical variogram model—such as spherical, exponential, or Gaussian—and estimating its key parameters, including the nugget, sill, and range. This procedure is often partly subjective, demands experience and judgment, and can be sensitive to outliers. Since prediction quality depends directly on the variogram specification, a poorly fitted model can result in unreliable estimates. Moreover, as a linear method, Kriging has limited ability to represent highly nonlinear spatial patterns and may overfit when applied to small datasets. It also implicitly assumes Gaussianity of the data, making transformations necessary when variables are strongly skewed.

Taken together, these limitations imply that although Kriging is theoretically optimal under its assumptions, it is not always the most practical choice in real-world settings. The gap between theoretical optimality and empirical performance is therefore a central theme in the benchmark results presented in this study.

These limitations motivate the exploration of alternative approaches that relax such assumptions while retaining strong predictive performance.


#### Machine Learning Methods

The machine learning (ML) approaches considered in this benchmark represent a fundamentally different paradigm from both deterministic and geostatistical methods. Rather than relying on fixed mathematical rules or explicit stochastic models of spatial dependence, ML algorithms are **data-driven** and learn predictive relationships directly from the data by optimizing a loss function. This property makes them highly flexible, scalable, and particularly effective in high-dimensional settings.

Within the ML family, we distinguish **two main classes of methods**: **tree-based ensemble methods** and **Generalized Additive Models (GAM)**. This distinction is important, as it reflects different modeling philosophies and leads to markedly different behaviors when applied to spatial data.


##### Tree-based Ensemble Methods

Tree-based ensemble methods are among the most widely used machine learning algorithms for tabular data and are known for their **state-of-the-art predictive performance**, **robustness, and scalability**. In this benchmark, we focus on two major subfamilies: **Random Forest (RF)** and **Gradient Boosting (GB)** methods.

Random Forest is based on a **bagging strategy**, where multiple decision trees are trained on **bootstrap samples** of the data and their predictions are averaged. Each tree partitions the feature space through **recursive binary splits** designed to **minimize prediction error within each region**. The aggregation of many decorrelated trees results in stable predictions with reduced variance.

Gradient Boosting methods, such as **XGBoost** and **MI-GBT**, adopt a different strategy. Trees are built **sequentially**, with each new tree trained to correct the residual errors of the existing ensemble. This iterative refinement **often** yields **higher predictive accuracy** than bagging-based methods, at the cost of increased **sensitivity to hyperparameter tuning** and a **higher risk of overfitting** if not properly regularized.

A key advantage of tree-based methods lies in their broad **applicability across a wide range of prediction tasks**, as they can be employed **without relying on strong assumptions** about the underlying data-generating process. Moreover, they are highly extensible, allowing additional covariates to be seamlessly incorporated into the model. For instance, in applications such as housing price prediction, spatial coordinates can be combined with socioeconomic or structural variables, a level of flexibility that is difficult to achieve with traditional geostatistical approaches.



#### Generalized Additive Models

Generalized Additive Models represent a distinct class within the ML family. GAMs model the target variable as a **sum of smooth nonlinear functions of the input features**, balancing flexibility and interpretability. While not tree-based, GAMs can capture nonlinear spatial trends through smooth functions of the coordinates, positioning them at the boundary between classical statistical modeling and modern machine learning. Their additive structure makes them more interpretable than ensemble methods, but also limits their ability to represent highly complex spatial interactions.


#### Adapting Tree-based Methods to Geospatial Data

Despite their strong performance in many domains, tree-based methods are not specifically designed for geospatial data. When applied naively to spatial coordinates, they face several well-known challenges inherent to spatial processes, including spatial autocorrelation, anisotropy, and the presence of large-scale spatial trends.

Spatial autocorrelation refers to the tendency of nearby observations to exhibit similar values. For example, environmental variables such as air pollution or temperature often display strong local correlation, meaning that observations are highly dependent on their neighbors. Standard tree-based models do not explicitly account for this dependence structure and may therefore overfit local noise. Similarly, many spatial phenomena display anisotropy, where dependence varies with direction rather than distance alone, as in the case of pollutant dispersion driven by prevailing winds or hydrological variables constrained by river networks. Axis-aligned decision trees struggle to capture such directional patterns efficiently, often requiring deep trees and many splits to approximate oblique spatial gradients.

Large-scale spatial trends present a particularly important challenge for tree-based methods. These trends represent smooth, gradual variations that span the entire spatial domain, such as elevation decreasing from mountainous regions to lowlands, temperature declining with latitude or altitude, or housing prices increasing radially from urban centers. Tree-based models approximate such trends using axis-aligned rectangular partitions, which is inherently inefficient. Capturing a smooth diagonal or curved trend requires many stepwise splits, leading to deep trees, high model complexity, and potential overfitting to local variations while failing to capture the global pattern effectively.

As a consequence, achieving optimal performance with tree-based methods in geospatial settings typically requires adaptation. Broadly speaking, there are two alternative strategies to address these challenges. The first strategy consists in modifying the algorithms themselves to make them more suitable for spatial data. Examples include Geographic Random Forest, which explicitly incorporates spatial information into the ensemble construction, and Oblique Random Forest, which allows decision boundaries that are not constrained to be parallel to the coordinate axes. By enabling oblique splits, these methods can more naturally represent spatial gradients that are misaligned with the original coordinate system.

The second strategy focuses on preprocessing the spatial data in a way that makes it better suited to standard tree-based algorithms, without altering their internal functioning. A prominent example of this approach is coordinate rotation, which augments the feature space with rotated versions of the original spatial coordinates. This transformation allows axis-aligned trees to effectively learn oblique spatial patterns, mitigating the geometric limitations of standard decision trees while preserving their computational efficiency and scalability.

In this benchmark, we investigate both adaptation strategies, with particular emphasis on coordinate rotation as a simple yet powerful preprocessing technique.

## Coordinate Rotation

#### Addressing Anisotropy: Oblique Trees versus Coordinate Rotation

Regarding spatial interpolation, all standard tree-based ensemble methods share a fundamental limitation: they rely exclusively on axis-aligned splits during tree construction. At each node, the algorithm selects a single feature and a threshold value to partition the data along that coordinate axis. This implies that when dealing with spatial data, tree-based models produce geographical splits with either a North-South (vertical) orientation or a (horizontal) East-West orientation. Obviously, spatial phenomena may have any spatial orientation, implying that off-the-shelf tree-based models must approximate diagonal spatial patterns by an accumulation of horizontal or vertical splits, leading to a staircase approximation.

To overcome these limitations, two main strategies can be considered. The first one is represented by Oblique Decision Trees, an algorithms that directly learn oblique splits by constructing linear combinations of features at each node; however, these methods are computationally demanding. The second strategy is our Coordinate Rotation, a simpler approach that augments the feature space with rotated versions of the original coordinates, allowing standard axis-aligned algorithms to effectively learn oblique boundaries.

The Coordinate rotation method is based on the transformation of the original spatial coordinates $(x, y)$ by applying a series of rotation transformation around the centroid of the training data. Using this procedure we are able to obtain an icreased number of feature space containing multiple and different representations of the same space locations, unique for their rotation angle. The precise procedure is detailed in the following pseudocode.

\begin{algorithm}
\caption{Spatial Feature Augmentation by Coordinate Rotation}
\begin{algorithmic}[1]
\Require Training dataset $\{(x_i, y_i)\}_{i=1}^n$, number of axes $k$
\Ensure Augmented feature matrix with original and rotated coordinates

\State Compute centroid:
\Statex \quad $\bar{x} \gets \frac{1}{n} \sum_{i=1}^n x_i$ \quad $\bar{y} \gets \frac{1}{n} \sum_{i=1}^n y_i$

\State Generate rotation angles and apply rotation:
\For{$j = 0$ to $k-1$}
    \State $\theta_j \gets \frac{360^\circ \cdot j}{k}$
    \For{$i = 1$ to $n$}
        \State $x'_{ij} \gets \bar{x} + (x_i - \bar{x}) \cos(\theta_j) - (y_i - \bar{y}) \sin(\theta_j)$
        \State $y'_{ij} \gets \bar{y} + (x_i - \bar{x}) \sin(\theta_j) + (y_i - \bar{y}) \cos(\theta_j)$
    \EndFor
\EndFor

\State Return augmented features: $\{(x'_{i0}, y'_{i0}, x'_{i1}, y'_{i1}, \ldots, x'_{i,k-1}, y'_{i,k-1})\}_{i=1}^n$

\end{algorithmic}
\end{algorithm}



### Model Selection for Coordinate Rotation

Not all algorithms benefit equally from coordinate rotation. For this reason, we apply the technique selectively based on each algorithmic properties:

**Models with Coordinate Rotation Variants**: 
- Random Forest (RF vs RF-CR) - XGBoost (XGB vs XGB-CR) - MI-GBT (MI-GBT vs MI-GBT-CR) - GAM (GAM vs GAM-CR)

The remaining algorithm are used only for a comparative purpose. This allow us to see how the coordinate rotations based models perform with respect to other existing strategy and isolate the specific impact of coordinate rotation on standard axis-aligned tree algorithms. 

### Geometric Interpretation

From a geometric perspective, coordinate rotation can be interpreted as a change of reference system applied to the spatial domain. Each rotation defines a new coordinate system whose axes are oriented at a specific angle with respect to the original one. Within these rotated systems, spatial patterns that appear oblique in the original coordinates may become aligned with the axes, allowing tree-based models to represent them through simple axis-aligned splits.

# Experimental Setup and Evaluation Metrics

## Datasets

There will be 8 datasets to cover combinations of:

-   **Real or Synthetic**
-   **Large or Small**
-   **Grid or No Grid**

For the **synthetic ones**, they are built following the idea that we need some data spatially correlated, be able to respect our different criteria:

-   **Spatial Correlation**: We use a **Matérn covariance model** (dimension=2, variance=1, length scale=10) to generate a Stationary Random Field (SRF). This ensures the synthetic data mimics the spatial continuity and "smoothness" often found in real-world environmental phenomena.
-   **Structure**: If there is a grid, points are generated on a regular Cartesian grid using a meshgrid of and coordinates. If it's not the case, points are sampled using a\*Uniform Random Distribution across the spatial domain to simulate irregular sampling.
-   **Size**: Ranging from 10,000 points for "Small" Datasets to 1,000,000 points for "Large" datasets
-   **Consistency**: A fixed seed (20170519) is applied to both the random field generation and the coordinate sampling to ensure the experiments are fully reproducible across different benchmark runs.

For the **real datasets**, we utilize high-quality topographic data provided by the **IGN (Institut National de l'Information Géographique et Forestière)**, the French national mapping agency.

-   **BD ALTI**: This dataset represents the "unstructured" real-world scenario. The points are derived from various sources (photogrammetry, digitization, etc.) where the spatial distribution of samples is irregular. So we can use this dataset as our no grid, large, real dataset.
-   **RGE ALTI**: It is the highest resolution elevation model available nationally. It is provided as a 5-meter regular grid. The full national dataset contains over 22 billion points. So we can use this dataset as our grid, large, real dataset.

For the Small real-world datasets, we use a subset of the French territory by filtering for Department 48 (Lozère). This department was chosen because its diverse topography—ranging from deep canyons and plateaus to mountainous terrain—offers a representative sample of various geographic challenges for spatial interpolation.

## Dataset Reference Table

| Dataset Name | Origin | Size Category | Structure | Approx. Row Count | Description |
|------------|------------|------------|------------|------------|------------|
| **bdalti** | Real | Large | No Grid | \~7,000,000 | BDALTI dataset. |
| **bdalti_48** | Real | Small | No Grid | \~40,000 | Department 48 (Lozère) subset of BDALTI. |
| **rgealti** | Real | Large | Grid | \~22,000,000,000 | RGEALTI. |
| **rgealti_48** | Real | Small | Grid | \~1,500,000 | Department 48 subset of RGEALTI. |
| **S-G-Sm** | Synthetic | Small | Grid | 10,000 | Structured Grid. |
| **S-G-Lg** | Synthetic | Large | Grid | 1,000,000 | Structured Grid. |
| **S-NG-Sm** | Synthetic | Small | No Grid | 10,000 | 10k points, Uniform Random Distribution. |
| **S-NG-Lg** | Synthetic | Large | No Grid | 1,000,000 | 1M points, Uniform Random Distribution. |


## Performance Metrics

We evaluate all models using three complementary metrics computed on the held-out test set:

-   **R² (Coefficient of Determination)**: Measures the proportion of variance in the target variable explained by the model. This metric is scale-invariant and provides an intuitive measure of overall model fit.

-   **RMSE (Root Mean Squared Error)**: Quantifies prediction error in the original units of the target variable. Penalizes large errors more heavily than small ones due to the squaring operation. Lower values indicate better performance.

-   **MAE (Mean Absolute Error)**: Measures average absolute prediction error in original units. More robust to outliers than RMSE. Lower values indicate better performance.

-   **Training Time**: Wall-clock time (in seconds) required for model fitting on the training set. Provides insight into computational efficiency and practical scalability.



## Benchmark Procedure   

All experiments ran in a Python 3.13 environment on SSPCloud with full parallelization (n_jobs=-1). To ensure reproducibility, a fixed random seed of 42 was applied across all generators. Data preprocessing included log-transforming skewed elevation targets and applying a 23-angle Coordinate Rotation (CR) for tree-based models to mitigate orthogonal split biases.

Our validation strategy employs a robust Randomized Search with K-Fold Cross-Validation. The procedure iterates through datasets and algorithms (a double loop), randomly sampling hyperparameter spaces. Each configuration is evaluated across K folds, with the "Best Model" selected based on the lowest average RMSE. We adapted tuning intensity to dataset size: high-intensity search (20 iterations, 5-fold, 10-min timeout) for small data, and an efficiency-focused regime (5 iterations, 3-fold) for large data.


# Results

Our empirical evaluation follows a sequential screening strategy. We first assess all candidate algorithms on small-scale datasets (approximately 5,000 observations) to identify the optimal trade-off between prediction accuracy and computational efficiency. Algorithms demonstrating favorable computational characteristics are then evaluated on large-scale datasets (exceeding 100,000 observations) and scenarios with substantial measurement error, including both synthetically corrupted spatial fields and real-world data with inherent noise.

```{python}
#| echo: false
import json
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from pathlib import Path
from matplotlib.lines import Line2D
from matplotlib.patches import Patch

# ============================================================================
# CONFIGURATION
# ============================================================================

sns.set_style("white") 
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10

# --- 1. Load and Merge Data ---
# We try to load both small and large/noisy result files
results = []

def load_json_results(filename):
    path = Path(filename)
    if path.exists():
        with open(path, 'r') as f:
            data = json.load(f)
            # Support both list-of-dicts or dict-with-results-key format
            if isinstance(data, list): return data
            if isinstance(data, dict) and 'results' in data: return data['results']
    return []

# Load both files generated by the benchmark scripts
results.extend(load_json_results('results/results_small.json'))
results.extend(load_json_results('results/results_large_noisy.json'))

# Fallback for testing (remove in prod)
if not results and Path('example.json').exists():
    with open('example.json', 'r') as f:
        results = json.load(f)['results']

# --- 2. Definitions ---

# Dataset Keys
datasets = [
    'rgealti_48', 'rgealti', 'bdalti_48', 'bdalti', 
    'S-G-Sm', 'S-G-Lg', 'S-NG-Sm', 'S-NG-Lg',
    'S-G-Lg-N', 'S-NG-Lg-N', 'cal_housing'
]

# Model Groups
deterministic_models = ['knn_3', 'idw_p3']
geostatistical_models = ['kriging']
tree_models = ['random_forest', 'random_forest_cr', 'xgboost', 'xgboost_cr', 'mixgboost', 'mixgboost_cr']
advanced_tree_models = ['oblique_rf', 'geoRF']
gam_models = ['gam', 'gam_cr']

model_names = {
    'random_forest': 'RF', 'random_forest_cr': 'RF-CR',
    'xgboost': 'XGB', 'xgboost_cr': 'XGB-CR',
    'mixgboost': 'MixGB', 'mixgboost_cr': 'MixGB-CR',
    'oblique_rf': 'Oblique RF', 'geoRF': 'GeoRF',
    'knn_3': 'KNN-3', 'idw_p3': 'IDW-p3',
    'kriging': 'Kriging', 'gam': 'GAM', 'gam_cr': 'GAM-CR'
}

dataset_names = {
    'rgealti_48': 'Real Grid Small',
    'rgealti': 'Real Grid Large',
    'bdalti_48': 'Real No-Grid Small',
    'bdalti': 'Real No-Grid Large',
    'S-G-Sm': 'Synth Grid Small',
    'S-G-Lg': 'Synth Grid Large',
    'S-NG-Sm': 'Synth No-Grid Small',
    'S-NG-Lg': 'Synth No-Grid Large',
    'S-G-Lg-N': 'Synth Grid Large (Noisy)',
    'S-NG-Lg-N': 'Synth No-Grid Large (Noisy)',
    'cal_housing': 'Housing (Real Noisy)'
}

color_deterministic = '#2E86AB'
color_geostat = '#A23B72'
color_tree = '#F18F01'
color_advanced_tree = '#C73E1D'
color_gam = '#6A994E'

def get_model_color(model):
    if model in deterministic_models: return color_deterministic
    if model in geostatistical_models: return color_geostat
    if model in tree_models: return color_tree
    if model in advanced_tree_models: return color_advanced_tree
    if model in gam_models: return color_gam
    return '#888888'


```

## 1. Preliminary Screening on Small Datasets

We first examine the relationship between computational cost and prediction error across all algorithm families using four small-scale datasets. The scatter plot visualizes training time (logarithmic scale) against RMSE, where lower values on both axes indicate superior performance.

```{python}
#| echo: false

def plot_small_datasets_landscape():
    """
    Plot: Training Time (X) vs RMSE (Y) for Small Datasets.
    Color: Algorithm
    Shape: Dataset
    """
    small_ds = ['rgealti_48', 'bdalti_48', 'S-G-Sm', 'S-NG-Sm']
    
    ds_shapes = {
        'rgealti_48': 'o', 'bdalti_48': 's',
        'S-G-Sm': '^', 'S-NG-Sm': 'D'
    }
    
    plot_data = []
    
    for m_key in model_names.keys():
        for ds in small_ds:
            res = next((r for r in results if r['model'] == m_key and r['dataset'] == ds), None)
            if res:
                # Use RMSE directly if available, else calc from MSE
                val_rmse = res.get('rmse', np.sqrt(res.get('mse', 0)))
                
                plot_data.append({
                    'Model': model_names[m_key],
                    'ModelKey': m_key,
                    'Dataset': dataset_names[ds],
                    'DatasetKey': ds,
                    'Time': res.get('training_time', 0.001),
                    'RMSE': val_rmse,
                    'Color': get_model_color(m_key),
                    'Shape': ds_shapes[ds]
                })

    df = pd.DataFrame(plot_data)
    
    fig, ax = plt.subplots(figsize=(10, 7))
    
    for _, row in df.iterrows():
        ax.scatter(row['Time'], row['RMSE'], 
                   c=row['Color'], marker=row['Shape'], 
                   s=100, alpha=0.7, edgecolors='black', linewidth=0.5)

    ax.set_xscale('log')
    ax.set_xlabel('Training Time (seconds) - Log Scale')
    ax.set_ylabel('RMSE (Lower is Better)')
    ax.set_title('Small Datasets: Efficiency vs. Accuracy Trade-off')
    ax.grid(True, linestyle='--', alpha=0.3)

    # Legends
    legend_elements_color = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_deterministic, markersize=10, label='Deterministic'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_tree, markersize=10, label='Tree Ensemble'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_advanced_tree, markersize=10, label='Advanced Tree'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_geostat, markersize=10, label='Geostatistical'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_gam, markersize=10, label='GAM'),
    ]
    ax.legend(handles=legend_elements_color, title="Algorithm Family", loc='upper left')
    
    plt.tight_layout()
    plt.show()

plot_small_datasets_landscape()

```

The results reveal substantial heterogeneity in the efficiency-accuracy frontier. Kriging, while achieving competitive prediction error, exhibits training times three orders of magnitude higher than ensemble methods due to the O(n³) complexity of covariance matrix inversion. GAM and GeoRF similarly demonstrate prohibitive computational requirements even at this moderate sample size. Given that large-scale evaluation involves datasets 20× larger, these methods are excluded from further analysis.

The retained algorithms—comprising tree-based ensembles (RF, XGBoost, MixGBoost) and deterministic baselines (KNN, IDW)—strike a practical balance between statistical performance and scalability. These methods advance to evaluation on high-dimensional datasets.

## 2. Performance Analysis on Large-Scale Data

### 2.1 Computational Scalability

To assess algorithmic complexity in practice, we compare average training times between small (n ≈ 5,000) and large (n ≈ 100,000) datasets. The augmentation factor quantifies how training time scales with a 20-fold increase in sample size.

```{python}
#| echo: false
def create_table_time_scalability_v2():
    small_ds = ['rgealti_48', 'bdalti_48', 'S-G-Sm', 'S-NG-Sm']
    large_ds = ['rgealti', 'bdalti', 'S-G-Lg', 'S-NG-Lg']
    exclude_large = ['geoRF', 'gam', 'gam_cr', 'kriging'] 
    
    table_data = []
    
    for m_key in model_names.keys():
        # Use .get('training_time', 0) to prevent KeyErrors
        times_sm = [r.get('training_time', 0) for r in results if r['model'] == m_key and r['dataset'] in small_ds]
        times_lg = [r.get('training_time', 0) for r in results if r['model'] == m_key and r['dataset'] in large_ds]
        
        if times_sm:
            avg_sm = np.mean(times_sm)
            
            if m_key in exclude_large:
                avg_lg_str = "Too Slow"
                ratio_str = "-"
                sort_val = 9999.0
            elif times_lg:
                avg_lg = np.mean(times_lg)
                if avg_lg > 0:
                    avg_lg_str = f"{avg_lg:.2f}s"
                    ratio = avg_lg / avg_sm if avg_sm > 0 else 0
                    ratio_str = f"{ratio:.1f}x"
                    sort_val = avg_lg
                else:
                    # Time is 0 or missing in data
                    avg_lg_str = "N/A" 
                    ratio_str = "-"
                    sort_val = 9999.0
            else:
                avg_lg_str = "NA"
                ratio_str = "-"
                sort_val = 9999.0

            table_data.append({
                'Model': model_names[m_key],
                'Avg Time (Small)': f"{avg_sm:.3f}s",
                'Avg Time (Large)': avg_lg_str,
                'Augmentation Factor': ratio_str,
                'sort_helper': sort_val
            })
            
    df = pd.DataFrame(table_data)
    df = df.sort_values('sort_helper').drop('sort_helper', axis=1)
    
    return df

display(create_table_time_scalability_v2())

```

Deterministic methods maintain near-constant training times regardless of dataset size, consistent with their distance-based interpolation schemes. Tree ensemble methods exhibit moderate scaling, with augmentation factors typically between 15× and 25×, slightly sublinear relative to the data increase due to early stopping and sampling strategies in boosting algorithms.

### 2.2 Effect of Coordinate Rotation on Prediction Accuracy

Standard tree-based algorithms partition the feature space using axis-aligned splits, which inefficiently represent oblique spatial patterns. Coordinate rotation addresses this limitation by augmenting the feature space with rotated coordinate systems, enabling trees to capture diagonal gradients through simpler decision boundaries.

The figure compares RMSE for standard implementations versus coordinate-rotated variants across four large datasets. Points below the identity line indicate that coordinate rotation reduces prediction error.

```{python}
#| echo: false
def create_fig_cr_impact_large():
    large_ds = ['rgealti', 'bdalti', 'S-G-Lg', 'S-NG-Lg']
    
    tree_pairs = [
        ('random_forest', 'random_forest_cr', 'o', 'RF'), 
        ('xgboost', 'xgboost_cr', 's', 'XGB'), 
        ('mixgboost', 'mixgboost_cr', 'D', 'MixGB')
    ]
    
    plot_data = []
    for std_key, cr_key, marker, name in tree_pairs:
        for ds in large_ds:
            std_res = next((r for r in results if r['model'] == std_key and r['dataset'] == ds), None)
            cr_res = next((r for r in results if r['model'] == cr_key and r['dataset'] == ds), None)
            
            if std_res and cr_res:
                # Use .get() to avoid KeyError if the metric is missing
                std_r2 = std_res.get('rmse')
                cr_r2 = cr_res.get('rmse')
                
                # Only plot if both scores exist and are valid numbers
                if std_r2 is not None and cr_r2 is not None:
                    plot_data.append({
                        'Standard': std_r2,
                        'Rotated': cr_r2,
                        'Algorithm': name,
                        'Marker': marker
                    })

    if not plot_data:
        # Fallback if no data is found (to avoid empty plot errors)
        return

    df = pd.DataFrame(plot_data)
    fig, ax = plt.subplots(figsize=(8, 8))
    
    ax.plot([0.4, 1], [0.4, 1], color='red', linestyle='--', alpha=0.5, label='Equality (y=x)')
    
    for name, marker in [('RF', 'o'), ('XGB', 's'), ('MixGB', 'D')]:
        sub = df[df['Algorithm'] == name]
        if not sub.empty:
            ax.scatter(sub['Standard'], sub['Rotated'], marker=marker, color=color_tree, 
                       s=120, edgecolor='black', label=name, alpha=0.9)
    
    ax.set_title('Impact of CR on Large Datasets', fontweight='bold')
    ax.set_xlabel('$R^2$ (Standard)')
    ax.set_ylabel('$R^2$ (Rotated / CR)')
    ax.legend(title="Algorithm")
    ax.grid(True, alpha=0.3)
    plt.show()

create_fig_cr_impact_large()

```

The magnitude of improvement is modest but consistent. On clean synthetic fields, coordinate rotation reduces RMSE by approximately 3-7% across tree-based methods. Random Forest shows the smallest gains (3-4% reduction), while XGBoost and MixGBoost achieve slightly larger improvements (5-7%). However, it is important to recognize that standard implementations already achieve low prediction errors (RMSE < 0.15 on normalized data), making further reductions incrementally valuable. The benefit persists across both gridded and irregularly sampled configurations, suggesting the mechanism is not artifact of specific sampling designs.

### 2.3 Robustness to Measurement Error

Real spatial datasets often contain substantial noise from sensor error, spatial aggregation, or intrinsic variability. We evaluate algorithmic stability by comparing performance on clean synthetic fields versus versions corrupted with additive Gaussian noise (signal-to-noise ratio approximately 2:1). The California Housing dataset provides a complementary real-world scenario with inherent non-spatial confounding.

```{python}
#| echo: false
def create_table_noisy_comparison():
    # Comparisons pairs
    clean_ds = ['S-G-Lg', 'S-NG-Lg']
    noisy_ds = ['S-G-Lg-N', 'S-NG-Lg-N']
    housing_ds = 'cal_housing'
    
    table_data = []
    
    # We focus on the key scalable models
    key_models = ['knn_3', 'random_forest', 'random_forest_cr', 'xgboost', 'xgboost_cr']
    
    for m_key in key_models:
        # Calculate Average RMSE for Clean vs Noisy Synthetic
        rmse_clean_vals = [r.get('rmse', 0) for r in results if r['model'] == m_key and r['dataset'] in clean_ds]
        rmse_noisy_vals = [r.get('rmse', 0) for r in results if r['model'] == m_key and r['dataset'] in noisy_ds]
        
        # Get Housing Score
        housing_res = next((r for r in results if r['model'] == m_key and r['dataset'] == housing_ds), None)
        housing_rmse = f"{housing_res['rmse']:.3f}" if housing_res else "NA"
        
        if rmse_clean_vals and rmse_noisy_vals:
            avg_clean = np.mean(rmse_clean_vals)
            avg_noisy = np.mean(rmse_noisy_vals)
            # How much did error increase?
            degradation = ((avg_noisy - avg_clean) / avg_clean) * 100
            
            table_data.append({
                'Model': model_names[m_key],
                'RMSE (Clean)': f"{avg_clean:.3f}",
                'RMSE (Noisy)': f"{avg_noisy:.3f}",
                'Noise Degradation': f"+{degradation:.1f}%",
                'Housing RMSE': housing_rmse
            })
            
    df = pd.DataFrame(table_data)
    return df

display(create_table_noisy_comparison())

```

Noise sensitivity varies substantially across algorithm families. KNN-3 shows catastrophic degradation (+307%), as expected from a non-parametric method without smoothing. Tree ensemble methods degrade less severely (180-265%), though this still represents a tripling or quadrupling of prediction error—hardly robust performance.

Coordinate rotation provides marginal benefit in noisy settings. On the California Housing dataset—where coordinates alone are weak predictors due to substantial non-spatial confounding—RF-CR achieves RMSE of 0.510 versus 0.545 for standard RF, a 6.4% reduction. While statistically consistent, this improvement is modest in practical terms. All methods perform poorly on this dataset (RMSE > 0.5), underscoring that spatial interpolation cannot substitute for properly specified predictive models when relevant covariates are excluded.

The persistence of coordinate rotation's benefit across noise conditions suggests it improves feature representation rather than merely exploiting clean data artifacts. However, the absolute gains remain small (typically 5-7% RMSE reduction), and practitioners should weigh whether this justifies the additional preprocessing complexity in their specific application context.

## Discussion

The training time analysis confirms that deterministic models like IDW and KNN are nearly instantaneous, while advanced tree-based models and GAMs carry significantly higher computational burden. The scalability analysis shows that ensemble methods scale reasonably to large datasets, though their axis-aligned split logic creates artifacts on oblique spatial patterns without coordinate rotation. Performance across grid and non-grid structures reveals this geometric limitation clearly.

Overall, CR-enhanced ensembles achieve modestly lower RMSE than standard implementations , while deterministic methods maintain competitive performance without requiring coordinate augmentation. The practical significance of these gains depends on application context: in high-stakes prediction where marginal improvements matter, coordinate rotation offers a low-cost enhancement; for exploratory analysis or rapid prototyping, standard implementations suffice.

The central question was whether modern ensemble methods could replace traditional spatial interpolators. The answer is nuanced. Coordinate rotation enables tree-based methods to match or slightly exceed kriging on computational grounds (100× faster) while achieving comparable prediction error on large datasets. However, the improvements over standard tree implementations are modest—typically 5-7% RMSE reduction—and come at the cost of feature engineering overhead.

Deterministic methods (KNN, IDW) remain highly competitive for rapid prototyping and applications where interpretability matters. They require no hyperparameter tuning, no training phase, and produce results instantly. Their prediction errors are only marginally higher than optimized ensemble methods in many scenarios. Therefore, the "best" algorithm depends critically on problem constraints: ensemble methods with coordinate rotation offer the highest accuracy when computational budget and implementation complexity permit; deterministic methods provide practical alternatives when simplicity, speed, or transparency dominate.

A significant limitation of this work is that the general datasets used may not capture the specific, high-frequency complex patterns where deterministic methods often excel due to their localized nature. Furthermore, the computational comparison is inherently biased because several algorithms rely on wraps for unmaintained or unoptimized packages, preventing them from reaching their true performance potential on our hardware. We are also limited by our reliance on  as the primary metric for the summary results. While  measures global variance, it can mask significant local errors that metrics like MAE or RMSE, which were calculated by the code but not fully explored in the main discussion, would reveal.


# Conclusion

The integration of **Coordinate Rotation (CR)** into ensemble methods allows for excellent results using algorithms that were not originally specialized for spatial interpolation tasks. This constitutes a promising discovery, especially given that these machine learning methods are continuously evolving and improving. However, to truly determine if they can consistently provide superior results, it is necessary to test them against datasets exhibiting more complex spatial patterns than those used in this general benchmark. Furthermore, while these ensemblist models are more time-consuming to compute compared to deterministic baselines, they prove to be highly robust as the dataset size increases. This scalability ensures they remain a viable and powerful option for high-density geographic modeling where traditional methods fail.

# References

All references cited in the report.

# Appendix A: Detailed Results

## Synthetic Small Grid

Detailed tables and figures.

## Synthetic Small No-Grid

Detailed tables and figures.

## Synthetic Large Grid

Detailed tables and figures.

## Synthetic Large No-Grid

Detailed tables and figures.

## Real Small Datasets

Detailed tables and figures.

## Real Large Datasets

Detailed tables and figures.

# Appendix B: Algorithm Parameters

Detailed hyperparameter settings for each algorithm.