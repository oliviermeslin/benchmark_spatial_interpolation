---
title: "Benchmark Algorithm of Ensemblist Methods on Spatial Data"
author:
  - "Bianco Andrea"
  - "Mancini Matteo"
  - "Mellot Rodrigue"
date: last-modified

format:
  html:
    toc: true
    number-sections: true
  pdf:
    documentclass: article
    papersize: a4
    number-sections: true
    colorlinks: true
    fig-pos: "H"

jupyter: python3
bibliography: references.bib

execute:
  enabled: true
---

# Abstract

Brief summary of the project, including: - the spatial interpolation problem, - the benchmarked methods, - the experimental design, - the main empirical results, - and the key conclusions.

# Introduction

All of the work produced in this report is being made with a fix seed equal to 42.

## Context

Introduce the spatial interpolation problem and its importance in environmental and geospatial applications.

## Research Question

Which algorithms perform best for spatial interpolation under different data-generating conditions?

## Motivation

Explain why comparing classical, geostatistical and machine learning approaches is relevant.

## Main Contributions

Summarize the main empirical and methodological contributions of the report.

## Structure of the Report

Brief description of the structure of the report.

# Datasets

There will be 8 datasets to cover combinations of:

-   **Real or Synthetic**
-   **Large or Small**
-   **Grid or No Grid**

For the **synthetic ones**, they are built following the idea that we need some data spatially correlated, be able to respect our different criteria:

-   **Spatial Correlation**: We use a **Matérn covariance model** (dimension=2, variance=1, length scale=10) to generate a Stationary Random Field (SRF). This ensures the synthetic data mimics the spatial continuity and "smoothness" often found in real-world environmental phenomena.
-   **Structure**: If there is a grid, points are generated on a regular Cartesian grid using a meshgrid of and coordinates. If it's not the case, points are sampled using a\*Uniform Random Distribution across the spatial domain to simulate irregular sampling.
-   **Size**: Ranging from 10,000 points for "Small" Datasets to 1,000,000 points for "Large" datasets
-   **Consistency**: A fixed seed (20170519) is applied to both the random field generation and the coordinate sampling to ensure the experiments are fully reproducible across different benchmark runs.

For the **real datasets**, we utilize high-quality topographic data provided by the **IGN (Institut National de l'Information Géographique et Forestière)**, the French national mapping agency.

-   **BD ALTI**: This dataset represents the "unstructured" real-world scenario. The points are derived from various sources (photogrammetry, digitization, etc.) where the spatial distribution of samples is irregular. So we can use this dataset as our no grid, large, real dataset.
-   **RGE ALTI**: It is the highest resolution elevation model available nationally. It is provided as a 5-meter regular grid. The full national dataset contains over 22 billion points. So we can use this dataset as our grid, large, real dataset.

For the Small real-world datasets, we use a subset of the French territory by filtering for Department 48 (Lozère). This department was chosen because its diverse topography—ranging from deep canyons and plateaus to mountainous terrain—offers a representative sample of various geographic challenges for spatial interpolation.

### Dataset Reference Table

| Dataset Name | Origin | Size Category | Structure | Approx. Row Count | Description |
|------------|------------|------------|------------|------------|------------|
| **bdalti** | Real | Large | No Grid | \~7,000,000 | BDALTI dataset. |
| **bdalti_48** | Real | Small | No Grid | \~400,000 | Department 48 (Lozère) subset of BDALTI. |
| **rgealti** | Real | Large | Grid | \~22,000,000,000 | RGEALTI. |
| **rgealti_48** | Real | Small | Grid | \~150,000 | Department 48 subset of RGEALTI. |
| **S-G-Sm** | Synthetic | Small | Grid | 10,000 | Structured Grid. |
| **S-G-Lg** | Synthetic | Large | Grid | 1,000,000 | Structured Grid. |
| **S-NG-Sm** | Synthetic | Small | No Grid | 10,000 | 10k points, Uniform Random Distribution. |
| **S-NG-Lg** | Synthetic | Large | No Grid | 1,000,000 | 1M points, Uniform Random Distribution. |

\*\* Note: For the full RGEALTI, the script currently uses a `.head(1_000_000)` limit to manage the massive 22-billion-row source file.\*

# Methodology

## General Framework

Overview of the benchmarking framework and experimental pipeline.

## Algorithms Considered

|  |  |
|----------------------------------------|--------------------------------|
| A\) Generalized Additive Models (GAM) | F\) Nearest Neighbor Interpolation |
| B\) GeoSpatial Random Forest | G\) Oblique Random Forest |
| C\) Gradient Boosting (HistGradientBoosting) | H\) Ordinary Kriging |
| D\) Inverse Distance Weighting (IDW) | I\) Random Forest |
| E\) MixGBoost | J\) XGBoost |

## Conceptual Comparison of Methods

### Algorithm Families: Theoretical Foundations

Our benchmark compares ten spatial interpolation algorithms that can be grouped into three distinct families based on their theoretical foundations: deterministic methods, geostatistical methods, and machine learning approaches. Understanding these distinctions is essential for interpreting our results and providing practical guidance for model selection.

#### Deterministic Methods

Inverse Distance Weighting (IDW) and K-Nearest Neighbors (K-NN) belong to the deterministic family. These methods share fundamental characteristics that distinguish them from other approaches. First, they make no probabilistic assumptions about the underlying data generation process. Second, they do not model prediction uncertainty or provide confidence intervals. Third, they are deterministic in the strict mathematical sense: given identical input data, they always produce the same output, with no stochastic component in their formulation. Finally, they rely on fixed mathematical rules to compute interpolated values rather than learning from data.

IDW calculates predictions as a weighted average based on inverse distance, where the weight assigned to each observation decreases as a power function of distance from the prediction location. The formula is straightforward: closer points receive higher weights, and the rate of decay is controlled by a single power parameter. KNN takes an even simpler approach, computing the average (or weighted average) of the k nearest neighbors to the prediction point. Both methods follow purely geometric rules without any underlying statistical model.

This simplicity brings clear practical advantages. Deterministic methods are easy to implement, requiring only basic distance calculations. They are computationally efficient since no training phase is needed—predictions can be made immediately from the observed data. The transparency of their mechanisms makes them highly interpretable: users can easily understand why a particular prediction was made. However, these strengths come with limitations. Deterministic methods do not explicitly model the complex spatial structure that may exist in the data. They also make rigid assumptions—for instance, IDW assumes that influence always decreases monotonically with distance, which may not hold in all spatial contexts. Despite these constraints, their robustness and speed make them valuable baseline methods.

#### Geostatistical Methods

Ordinary Kriging represents the geostatistical approach, which is rooted in the theory of regionalized variables. Unlike deterministic methods, Kriging treats spatial data as realizations of an underlying stochastic spatial process. This probabilistic framework enables the method to explicitly model spatial autocorrelation, provide uncertainty estimates for predictions, and optimize weights to minimize prediction error variance.

The heart of the Kriging approach is the variogram, a function that quantifies how the similarity between observations changes with distance. Rather than simply assuming that nearby points matter more (as deterministic methods do), Kriging asks a more nuanced question: exactly how much do nearby points matter, and how does this relationship change across different spatial scales? The variogram provides the answer by modeling the spatial correlation structure directly from the data. Through this analysis, Kriging determines optimal prediction weights by solving a system of linear equations that accounts for both the distances to observation points and the overall spatial configuration of the data.

This rigorous theoretical foundation gives Kriging several important strengths. It is proven to be the Best Linear Unbiased Predictor (BLUP) under its assumptions, meaning it provides optimal predictions in a well-defined statistical sense. The method explicitly models spatial structure rather than relying on simple distance-based rules. It guarantees exact interpolation at observed locations—the predicted value at any observation point equals the observed value. The resulting surfaces are smooth and continuous, avoiding the discontinuities that can occur with simpler methods. Perhaps most importantly, Kriging provides uncertainty quantification: it estimates not just the predicted value at each location, but also the prediction variance, enabling the construction of confidence intervals.

However, these theoretical advantages come with significant practical challenges. Kriging has cubic computational complexity, requiring the inversion of an n×n matrix where n is the number of observations. This makes it prohibitively slow for datasets exceeding roughly 10,000 points. The method also rests on strong statistical assumptions. Ordinary Kriging assumes intrinsic stationarity, meaning that the mean is constant (though unknown) across the study area and that the variogram depends only on the separation distance between points, not on their absolute locations. It further assumes isotropy, where spatial correlation depends only on distance, not direction. When spatial patterns show directional trends or anisotropy, these assumptions are violated, potentially degrading performance.

Beyond computational and theoretical constraints, Kriging faces practical implementation challenges. Before making predictions, one must first estimate the variogram from the data. This process involves choosing a theoretical variogram model (spherical, exponential, Gaussian, or others) and estimating its parameters (nugget, sill, and range). This estimation step is often somewhat subjective, requiring judgment and expertise, and the results can be sensitive to outliers in the data. An incorrectly specified variogram directly translates to poor predictions. Additionally, as a linear method, Kriging struggles to capture highly nonlinear spatial patterns. It can produce overfitted results when working with small datasets, and it implicitly assumes that the data follow a Gaussian distribution, requiring transformations when dealing with skewed variables.

These practical limitations mean that while Kriging is theoretically superior in many respects, it is not always the most practical choice for real-world applications. The gap between theoretical optimality and practical performance is an important theme in our benchmark results.

#### Machine Learning Methods

The machine learning family in our benchmark includes Random Forest, Oblique Random Forest, Geospatial Random Forest (GeoRF), XGBoost, HistGradientBoosting, MixGBoost, and Generalized Additive Models (GAM). These methods represent a fundamentally different paradigm from both deterministic and geostatistical approaches. Within this diverse group, we can distinguish tree-based ensemble methods (all except GAM) from non-tree-based approaches, a distinction that has important implications for their behavior with spatial data.

Machine learning algorithms are fundamentally data-driven. They do not rely on fixed mathematical formulas or assume specific probabilistic structures. Instead, they learn complex patterns directly from training data through an iterative optimization process that automatically minimizes a loss function. This learning process enables ML methods to model highly nonlinear relationships between input features (spatial coordinates x and y) and the target variable z, without requiring users to specify the functional form in advance. The models iteratively improve their predictions using optimization algorithms such as gradient descent, boosting, or bagging, adapting to whatever patterns exist in the training data.

Tree-based ensemble methods build predictions through recursive partitioning of the feature space. Random Forest employs a bagging strategy, training multiple decision trees on bootstrap samples of the data and averaging their predictions to reduce variance. Each individual tree learns to partition the coordinate space by making binary splits that minimize prediction error within resulting regions. The averaging across many diverse trees produces stable, robust predictions. Gradient boosting methods, including XGBoost, HistGradientBoosting, and MixGBoost, take a different approach by building trees sequentially. Each new tree is trained to predict the residual errors of the existing ensemble, with the final prediction being a weighted sum of all trees. This iterative error correction often leads to higher accuracy than bagging, though it requires careful tuning to avoid overfitting.

Oblique Random Forest extends the standard Random Forest framework by allowing decision boundaries that are not parallel to the coordinate axes. While standard trees make splits like "if x < threshold" or "if y < threshold", oblique trees can make diagonal splits such as "if a₁·x + a₂·y < threshold". This added flexibility can be particularly valuable for spatial data where patterns may not align with the coordinate system. GeoRF builds on Random Forest specifically for geographic applications, explicitly incorporating spatial information into the learning process while maintaining the ensemble tree framework.

GAM represents a distinct approach within the ML family. Rather than using trees, it models the target as a sum of smooth nonlinear functions of the input features. This additive structure maintains some interpretability while still capturing nonlinear effects, positioning GAM at the boundary between classical statistical modeling and modern machine learning.

Critically, none of these ML methods explicitly model spatial autocorrelation in the way that Kriging does. They do not estimate variograms or make assumptions about stationarity. Instead, they learn spatial patterns implicitly through the coordinate features provided during training, discovering whatever structure exists in the data through the optimization process.

This data-driven flexibility brings substantial advantages. ML methods can adapt to virtually any pattern in the data without requiring users to specify functional forms or make strong distributional assumptions. They scale well to large datasets—Random Forest and XGBoost can handle hundreds of thousands of observations efficiently. They automatically capture complex interactions between features, including nonlinear spatial patterns that would be difficult to specify manually. These methods represent the state-of-the-art in many prediction tasks across diverse domains.

However, this flexibility also introduces challenges. ML methods require a distinct training phase with careful attention to train-test splitting and cross-validation to avoid overfitting. They involve numerous hyperparameters (tree depth, number of estimators, learning rates, regularization strengths) that must be tuned, often requiring extensive experimentation. The resulting models are often difficult to interpret—understanding why a Random Forest with 100 trees makes a particular prediction is far less straightforward than understanding why IDW assigns certain weights. Unlike Kriging, ML methods do not naturally provide uncertainty estimates, though techniques like quantile regression or bootstrapping can be applied at additional computational cost.

Most importantly for spatial interpolation, tree-based ML methods face a fundamental geometric challenge. Decision trees make splits parallel to the coordinate axes, which works well when patterns align with these axes but struggles when spatial phenomena exhibit diagonal gradients. Elevation may increase from southwest to northeast, geological features may be oriented obliquely to map coordinates, or meteorological patterns may follow prevailing wind directions that do not align with north-south or east-west axes. Without additional preprocessing to address this limitation, tree-based methods can substantially underperform even simple deterministic approaches on spatial data.

### Synthesis: Choosing Among Families

The three algorithm families represent distinct trade-offs between theoretical rigor, computational efficiency, and practical flexibility. Deterministic methods offer unmatched simplicity and speed at the cost of limited sophistication. Geostatistical methods provide optimal predictions and uncertainty quantification under specific assumptions, but demand computational resources and statistical expertise. Machine learning methods deliver maximum flexibility and can capture complex patterns, but require careful training, tuning, and—for spatial applications—thoughtful preprocessing.

Our benchmark reveals that these theoretical distinctions translate into tangible performance differences that depend critically on the characteristics of the data and the appropriateness of preprocessing. The relationship between algorithmic complexity and predictive performance is not monotonic—more sophisticated methods do not automatically outperform simpler ones. Understanding when each family excels requires examining not just their theoretical properties, but their behavior on real spatial interpolation tasks.

## Coordinate Rotation

### Motivation

Tree-based models rely on axis-aligned splits, which are suboptimal for spatial data.

### Method

Application of coordinate rotations to enable oblique decision boundaries.

### Mathematical Formulation

$$
x' = x \cos(\theta) - y \sin(\theta)
$$

$$
y' = x \sin(\theta) + y \cos(\theta)
$$

### Implementation Details

-   Number of rotations\
-   Distribution of angles

## Experimental Setup and Evaluation Metrics

Our benchmark evaluation follows a standardized protocol to ensure fair comparison across all algorithms and datasets. The experimental configuration is summarized in Table X below.

**Table X: Benchmark Configuration**

| Component | Specification | Description |
|-----------|--------------|-------------|
| **Train/Test Split** | 80% / 20% | Training set for model fitting, test set for performance evaluation |
| **Random Seed** | 42 | Fixed seed for reproducible train/test splitting and model initialization |
| **Test Size** | Variable by dataset | Small datasets: 5,000 points; Large datasets: 100,000 points |
| **Coordinate Rotation** | 23 angles | Preprocessing for tree-based models (when applicable) |
| **Target Transformation** | Log transform | Applied to real elevation datasets (BDALTI, RGEALTI) to handle skewness |
| **Hardware** | SSPCloud environment | Multi-core CPU, optimized for parallel computation |
| **Software** | Python 3.x | scikit-learn, XGBoost, PyKrige, pyGAM, GeoRF, Polars, scikit-learn-intelex |
| **Parallelization** | n_jobs=-1 | All available cores utilized where supported |

**Performance Metrics:**

We evaluate all models using three complementary metrics computed on the held-out test set:

- **R² (Coefficient of Determination)**: Measures the proportion of variance in the target variable explained by the model. This metric is scale-invariant and provides an intuitive measure of overall model fit.

- **RMSE (Root Mean Squared Error)**: Quantifies prediction error in the original units of the target variable. Penalizes large errors more heavily than small ones due to the squaring operation. Lower values indicate better performance.

- **MAE (Mean Absolute Error)**: Measures average absolute prediction error in original units. More robust to outliers than RMSE. Lower values indicate better performance.

- **Training Time**: Wall-clock time (in seconds) required for model fitting on the training set. Provides insight into computational efficiency and practical scalability.

All metrics are computed on the test set to assess generalization performance on unseen data. For models requiring hyperparameters (e.g., variogram models for Kriging, number of estimators for tree-based methods), we use configurations established through preliminary tuning or standard recommendations from the literature. The focus of this benchmark is on comparing algorithm families rather than exhaustive hyperparameter optimization for individual models.

# Results
```{python}
#| echo: false
import json
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from pathlib import Path

# ============================================================================
# CONFIGURATION
# ============================================================================

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['axes.labelsize'] = 11
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['xtick.labelsize'] = 9
plt.rcParams['ytick.labelsize'] = 9

# Load data
with open('example.json', 'r') as f:
    data = json.load(f)

results = data['results']

# Define datasets and model groups
datasets = ['rgealti', 'bdalti', 'bdalti_48', 'rgealti_48', 'S-G-Sm', 'S-G-Lg', 'S-NG-Sm', 'S-NG-Lg']

# Model groupings
deterministic_models = ['knn_3', 'idw_p3']
geostatistical_models = ['kriging']
tree_models = ['random_forest', 'random_forest_cr', 'xgboost', 'xgboost_cr', 'mixgboost', 'mixgboost_cr']
advanced_tree_models = ['oblique_rf', 'geoRF']
gam_models = ['gam', 'gam_cr']

# Model display names
model_names = {
    'random_forest': 'RF',
    'random_forest_cr': 'RF-CR',
    'xgboost': 'XGB',
    'xgboost_cr': 'XGB-CR',
    'mixgboost': 'MixGB',
    'mixgboost_cr': 'MixGB-CR',
    'oblique_rf': 'Oblique RF',
    'geoRF': 'GeoRF',
    'knn_3': 'KNN-3',
    'idw_p3': 'IDW-p3',
    'kriging': 'Kriging',
    'gam': 'GAM',
    'gam_cr': 'GAM-CR'
}

# Dataset display names
dataset_names = {
    'rgealti': 'RGE ALTI',
    'bdalti': 'BD ALTI',
    'bdalti_48': 'BD ALTI 48',
    'rgealti_48': 'RGE ALTI 48',
    'S-G-Sm': 'Synth Grid Small',
    'S-G-Lg': 'Synth Grid Large',
    'S-NG-Sm': 'Synth No-Grid Small',
    'S-NG-Lg': 'Synth No-Grid Large'
}

# Color schemes
color_deterministic = '#2E86AB'
color_geostat = '#A23B72'
color_tree = '#F18F01'
color_advanced_tree = '#C73E1D'
color_gam = '#6A994E'

def get_model_color(model):
    """Get color for a model based on its family"""
    if model in deterministic_models:
        return color_deterministic
    elif model in geostatistical_models:
        return color_geostat
    elif model in tree_models:
        return color_tree
    elif model in advanced_tree_models:
        return color_advanced_tree
    elif model in gam_models:
        return color_gam
    return '#888888'
```

## Overall Performance

Global comparison of all algorithms across datasets.

```{python}
# 

def create_fig1_r2_heatmap():
    """Figure 1: R² Scores Heatmap"""
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Prepare data
    all_models = sorted(set([r['model'] for r in results]))
    r2_matrix = np.zeros((len(all_models), len(datasets)))
    r2_matrix[:] = np.nan
    
    for i, model in enumerate(all_models):
        for j, dataset in enumerate(datasets):
            matching = [r for r in results if r['model'] == model and r['dataset'] == dataset]
            if matching:
                r2_matrix[i, j] = matching[0]['r2_score']
    
    # Create heatmap
    im = ax.imshow(r2_matrix, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)
    
    # Set ticks
    ax.set_xticks(np.arange(len(datasets)))
    ax.set_yticks(np.arange(len(all_models)))
    ax.set_xticklabels([dataset_names.get(d, d) for d in datasets], rotation=45, ha='right')
    ax.set_yticklabels([model_names.get(m, m) for m in all_models])
    
    # Add colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('R² Score', rotation=270, labelpad=15)
    
    # Add text annotations
    for i in range(len(all_models)):
        for j in range(len(datasets)):
            if not np.isnan(r2_matrix[i, j]):
                text = ax.text(j, i, f'{r2_matrix[i, j]:.3f}',
                              ha="center", va="center", color="black", fontsize=7)
    
    ax.set_title('R² Scores Across All Models and Datasets')
    plt.tight_layout()
    plt.show()

create_fig1_r2_heatmap()

```

## Results by Algorithm Family

### Deterministic Methods

Analysis of IDW and nearest-neighbor interpolation.

### Geostatistical Methods

Analysis of Ordinary Kriging and related findings.

### Tree-Based Machine Learning Methods

Performance of Random Forest, Gradient Boosting, XGBoost and MixGBoost.

### Advanced Spatial Tree Methods

GeoSpatial RF and Oblique RF results.

### Generalized Additive Models

Performance and computational considerations.

## Cross-Dataset Analysis

### Effect of Coordinate Rotation

Comparison of results with and without rotation.

### Grid vs No-Grid Datasets

Impact of spatial structure on performance.

### Scalability

Performance comparison on small vs large datasets.

# Discussion

## Interpretation of Results

Explanation of observed performance patterns.

## Practical Implications

Guidelines for practitioners choosing interpolation methods.

## Limitations

Discussion of methodological and computational limitations.

## Future Work

Possible extensions and improvements.

# Conclusion

Summary of key findings and contributions.

# References

All references cited in the report.

# Appendix A: Detailed Results

## Synthetic Small Grid

Detailed tables and figures.

## Synthetic Small No-Grid

Detailed tables and figures.

## Synthetic Large Grid

Detailed tables and figures.

## Synthetic Large No-Grid

Detailed tables and figures.

## Real Small Datasets

Detailed tables and figures.

## Real Large Datasets

Detailed tables and figures.

# Appendix B: Algorithm Parameters

Detailed hyperparameter settings for each algorithm.