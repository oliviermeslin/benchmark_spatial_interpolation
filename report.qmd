title: "Benchmark Algorithm of Ensemblist Methods on Spatial Data"
author: "Bianco Andrea, Mancini Matteo, Mellot Rodrigue"
date: last-modified
format:
  pdf:
    documentclass: article
    papersize: a4
    number-sections: true
    colorlinks: true
    fig-pos: 'H'
jupyter: python3
bibliography: references.bib
---

# Introduction

# Methodology

## Algorithms Descriptions

We will compare plenty different algorithms that are commonly used in the literature to solve spatial problems. We will try them with or without coordinates rotation. 

1.  **Random Forest** 

2.  **Gradient Boosting** 

3.  **MI-GBT**

4.  **Oblique Trees**

5.  **GeoSpatial Random Forest**

6.  **Ordinary Kriging**

7.  **Inverse Distance Weighting (IDW)**

8.  **Nearest Neighbor Interpolation**

9.  **Generalized Additive Models (GAM)**

## Datasets

There will be 8 datasets to cover combinations of:

* **Real or Synthetic**
* **Large or Small**
* **Grid or No Grid**

For the **synthetic ones**, they are built following the idea that we need some data spatially correlated, be able to respect our different criteria:

* **Spatial Correlation**: We use a **Matérn covariance model** (dimension=2, variance=1, length scale=10) to generate a Stationary Random Field (SRF). This ensures the synthetic data mimics the spatial continuity and "smoothness" often found in real-world environmental phenomena.
* **Structure**: If there is a grid, points are generated on a regular Cartesian grid using a meshgrid of  and  coordinates. If it's not the case, points are sampled using a*Uniform Random Distribution across the spatial domain to simulate irregular sampling.
* **Size**: Ranging from 10,000 points for "Small" Datasets to 1,000,000 points for "Large" datasets
* **Consistency**: A fixed seed (20170519) is applied to both the random field generation and the coordinate sampling to ensure the experiments are fully reproducible across different benchmark runs.

For the **real datasets**, we utilize high-quality topographic data provided by the **IGN (Institut National de l'Information Géographique et Forestière)**, the French national mapping agency. 

* **BD ALTI**: This dataset represents the "unstructured" real-world scenario. The points are derived from various sources (photogrammetry, digitization, etc.) where the spatial distribution of samples is irregular. So we can use this dataset as our no grid, large, real dataset.
* **RGE ALTI**: It is the highest resolution elevation model available nationally. It is provided as a 5-meter regular grid. The full national dataset contains over 22 billion points. So we can use this dataset as our grid, large, real dataset.

For the Small real-world datasets, we use a subset of the French territory by filtering for Department 48 (Lozère). This department was chosen because its diverse topography—ranging from deep canyons and plateaus to mountainous terrain—offers a representative sample of various geographic challenges for spatial interpolation.

### Dataset Reference Table

| Dataset Name | Origin | Size Category | Structure | Approx. Row Count | Description |
| --- | --- | --- | --- | --- | --- |
| **bdalti** | Real | Large | No Grid | ~7,000,000 | BDALTI dataset. |
| **bdalti_48** | Real | Small | No Grid | ~400,000 | Department 48 (Lozère) subset of BDALTI. |
| **rgealti** | Real | Large | Grid | ~22,000,000,000 |  RGEALTI. |
| **rgealti_48** | Real | Small | Grid | ~150,000 | Department 48 subset of RGEALTI. |
| **S-G-Sm** | Synthetic | Small | Grid | 10,000 |  Structured Grid. |
| **S-G-Lg** | Synthetic | Large | Grid | 1,000,000 |  Structured Grid. |
| **S-NG-Sm** | Synthetic | Small | No Grid | 10,000 | 10k points, Uniform Random Distribution. |
| **S-NG-Lg** | Synthetic | Large | No Grid | 1,000,000 | 1M points, Uniform Random Distribution. |

** Note: For the full RGEALTI, the script currently uses a `.head(1_000_000)` limit to manage the massive 22-billion-row source file.*

## Experience

The goal of the Experience is to benchmark ie. compare these different algorithms. To do that,for each dataset, we will do a cross validation to evaluate :

* RMSE (Root Mean Squared Error)
* MAE (Mean Absolute Error)
* R2
* Executing Time

# Results

This section presents the empirical findings of our benchmark, focusing on predictive precision across metrics and computational efficiency.

## Performance Metrics Summary

The following table summarizes the average performance across all datasets. We observe that **Coordinate Rotation (CR)** significantly improves the predictive power of axis-aligned learners.

```{python}
#| echo: false
#| label: tbl-summary
#| tbl-cap: "Mean Performance Metrics across all Datasets"

import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Load the benchmark results
file_path = "/home/onyxia/work/benchmark_spatial_interpolation/results/benchmark_results.json"

with open(file_path, 'r') as f:
    data = json.load(f)

df = pd.DataFrame(data['results'])

# Data cleaning and feature engineering
df['CR'] = df['model'].apply(lambda x: "Yes" if x.endswith('_cr') else "No")
df['Algorithm'] = df['model'].str.replace('_cr', '').str.replace('_', ' ').str.title()

# Summary Table
summary = df.groupby(['Algorithm', 'CR'])[['r2_score', 'rmse', 'mae', 'training_time']].mean().reset_index()
summary.columns = ['Algorithm', 'CR', 'Mean R²', 'Mean RMSE', 'Mean MAE', 'Time (s)']
summary.sort_values('Mean R²', ascending=False).round(4)

```

## Comparative Analysis of Predictive Power

The following visualizations compare the precision of the models. The first two plots illustrate the distribution of  and RMSE, highlighting the "lift" provided by coordinate rotation.

```{python}
#| echo: false
#| label: fig-precision
#| fig-cap: "Distribution of R² and RMSE across all models and datasets."
#| fig-subcap: 
#|   - "R² Score Comparison"
#|   - "RMSE Comparison"
#| layout-ncol: 2

sns.set_theme(style="whitegrid")

# Plot 1: R2 Score
plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='Algorithm', y='r2_score', hue='CR', palette='viridis')
plt.title('R² Score Comparison (CR vs. Standard)')
plt.xticks(rotation=45)
plt.ylabel('R² Score')
plt.show()

# Plot 2: RMSE
plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='Algorithm', y='rmse', hue='CR', palette='magma')
plt.title('RMSE Comparison (Lower is Better)')
plt.xticks(rotation=45)
plt.ylabel('RMSE')
plt.show()

```

The third plot measures the specific impact of Coordinate Rotation by calculating the delta () in  score ().

```{python}
#| echo: false
#| label: fig-cr-delta
#| fig-cap: "Impact of Coordinate Rotation on Predictive Performance (Delta R²)."

# Pivot to calculate Delta
df_pivot = df.pivot_table(index=['dataset', 'Algorithm'], columns='CR', values='r2_score').reset_index()
df_pivot['R2 Delta'] = df_pivot['Yes'] - df_pivot['No']

plt.figure(figsize=(10, 5))
sns.boxplot(data=df_pivot, x='Algorithm', y='R2 Delta', color='skyblue')
plt.axhline(0, color='red', linestyle='--')
plt.title('Coordinate Rotation Impact: ΔR²')
plt.ylabel('Change in R² Score')
plt.xticks(rotation=45)
plt.show()

```

## Computational Efficiency

The training time is a critical bottleneck for large-scale spatial interpolation. @fig-training-time illustrates the execution time on a log scale, highlighting the extreme cost of models like GAM with CR compared to efficient neighbors like KNN.

```{python}
#| echo: false
#| label: fig-training-time
#| fig-cap: "Training Time Comparison on Large Datasets (Log Scale)."

plt.figure(figsize=(10, 6))
# Filter for large datasets to show scaling
df_lg = df[df['dataset'].str.contains('Lg')]

sns.barplot(data=df_lg, x='model', y='training_time', palette='coolwarm')
plt.yscale('log')
plt.title('Training Time (Dataset: Large, Log Scale)')
plt.ylabel('Time (seconds)')
plt.xticks(rotation=90)
plt.show()

```

## Discussion of Findings
 BLABLABLA

# Conclusion

# References

::: {\#refs}
:::
